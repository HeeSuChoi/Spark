{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"]=os.path.join(os.path.expanduser(\"~\"),'spark-2.0.0-bin-hadoop2.7')\n",
    "os.environ[\"PYLIB\"]=os.path.join(os.environ[\"SPARK_HOME\"],'python','lib')\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.1-src.zip'))\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'pyspark.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config('spark.sql.warehouse.dir','C:\\Users\\qorgk\\code\\spark')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sparkㄱㅏ ㅅㅡㅋㅏㄹㄹㅏㄹㅗ ㅁㅏㄴㄷㅡㄹㅇㅓ ㅈㅕㅆㄱㅣ ㄸㅐㅁㅜㄴㅇㅔ\n",
    "ㅍㅏㅇㅣㅆㅓㄴㅇㅣㅈㅣㅁㅏㄴ ㄷㅔㅇㅣㅌㅓ ㅌㅏㅇㅣㅍㅇㅡㄹ ㅅㅣㄴㄱㅕㅇㅆㅓㅅㅓ ㅅㅏㅇㅛㅇㅎㅐㅇㅑㅎㅏㄴㄷㅏ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "myList=[('1','kim, js',170),\n",
    "        ('1','lee, sm', 175),\n",
    "        ('2','lim, yg',180),\n",
    "        ('2','lee',170)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf = spark.createDataFrame(myList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ㅋㅓㄹㄹㅓㅁㅁㅕㅇㅇㅣ ㅈㅏㄷㅗㅇㅇㅡㄹㅗ ㅈㅜㅇㅓㅈㅣㅁ ㄷㅔㅇㅣㅌㅓ ㅌㅏㅇㅣㅍㄷㅗ ㅈㅏㄷㅗㅇㅇㅡㄹㅗ ㅇㅠㅊㅜㄷㅗㅣㅁ nullableㅇㅡㄴ ㅅㅔㄹㄱㅏㅂㅅㅇㅣ ㅂㅣㅇㅓㅇㅣㅆㅇㅡㄹ ㅅㅜㄷㅗ ㅇㅣㅆㄷㅏㄴㅡㄴ ㄸㅡㅅㅇㅣㅁ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(_1=u'1', _2=u'kim, js', _3=170)]\n"
     ]
    }
   ],
   "source": [
    "print myDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(year=u'1', name=u'kim, js', height=170)]\n"
     ]
    }
   ],
   "source": [
    "print spark.createDataFrame(myList, ['year','name','height']).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|name|      item|\n",
      "+----+----------+\n",
      "| kim|  espresso|\n",
      "| lee|     latte|\n",
      "| lee| americano|\n",
      "| lim|  affocato|\n",
      "| kim|long black|\n",
      "| lee|  macciato|\n",
      "| lee|  espresso|\n",
      "| lim|     latte|\n",
      "| kim| americano|\n",
      "| lee|  affocato|\n",
      "+----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names = [\"kim\",\"lee\",\"lee\",\"lim\"]\n",
    "items = [\"espresso\",\"latte\",\"americano\",\"affocato\",\"long black\",\"macciato\"]\n",
    "df = spark.createDataFrame([(names[i%4], items[i%6]) for i in range(100)],\\\n",
    "                           [\"name\",\"item\"])\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|short name|\n",
      "+----------+\n",
      "|       esp|\n",
      "|       lat|\n",
      "|       ame|\n",
      "+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.item.substr(1, 3).alias(\"short name\")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row1:  1 kim, js\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "Person = Row('year','name', 'height')\n",
    "row1=Person('1','kim, js',170)\n",
    "print \"row1: \", row1.year, row1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRows = [row1,\n",
    "          Person('1','lee, sm', 175),\n",
    "          Person('2','lim, yg',180),\n",
    "          Person('2','lee',170)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf=spark.createDataFrame(myRows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: long (nullable = true)\n",
      "\n",
      "None\n",
      "+----+-------+------+\n",
      "|year|   name|height|\n",
      "+----+-------+------+\n",
      "|   1|kim, js|   170|\n",
      "|   1|lee, sm|   175|\n",
      "|   2|lim, yg|   180|\n",
      "|   2|    lee|   170|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print myDf.printSchema()\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "mySchema=StructType([\n",
    "    StructField(\"year\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"height\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(year=u'1', name=u'kim, js', height=170)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf=spark.createDataFrame(myRows, mySchema)\n",
    "myDf.printSchema()\n",
    "myDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "myList=[('1','kim, js',170),('1','lee, sm', 175),('2','lim, yg',180),('2','lee',170)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRdd = spark.sparkContext.parallelize(myList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddDf=myRdd.toDF()\n",
    "rddDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddDf=spark.createDataFrame(myRdd)\n",
    "rddDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| _1|     _2|\n",
      "+---+-------+\n",
      "|  1|kim, js|\n",
      "|  2|    lee|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddDf.where(rddDf._3 < 175).select([rddDf._1, rddDf._2]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where ㅎㅐㅇ\n",
    "select ㅇㅕㄹ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| _1|max(_3)|\n",
      "+---+-------+\n",
      "|  1|    175|\n",
      "|  2|    180|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddDf.groupby(rddDf._1).max().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "byㄴㅡㄴ keyㅂㅕㄹ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "_myRdd=myRdd.map(lambda x:Row(year=int(x[0]),name=x[1],height=int(x[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "_myDf=spark.createDataFrame(_myRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- height: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(height=170, name=u'kim, js', year=1)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_myDf.printSchema()\n",
    "_myDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType, TimestampType\n",
    "r1=Row(name=\"js1\",age=10)\n",
    "r2=Row(name=\"js2\",age=20)\n",
    "_myRdd=spark.sparkContext.parallelize([r1,r2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=10, name='js1'), Row(age=20, name='js2')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_myRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 10| js1|\n",
      "| 20| js2|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema=StructType([\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    #StructField(\"created\", TimestampType(), True)\n",
    "])\n",
    "_myDf=spark.createDataFrame(_myRdd,schema)\n",
    "_myDf.printSchema()\n",
    "_myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      "\n",
      "+---+----+------+\n",
      "| id|name|height|\n",
      "+---+----+------+\n",
      "|  1| kim|  50.0|\n",
      "|  2| lee|  60.0|\n",
      "|  3|park|  70.0|\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "myRdd=spark.sparkContext.parallelize([(1, 'kim', 50.0), (2, 'lee', 60.0), (3, 'park', 70.0)])\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"height\", DoubleType(), True)\n",
    "])\n",
    "_myDf = spark.createDataFrame(myRdd, schema)\n",
    "_myDf.printSchema()\n",
    "_myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>name</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>kim, js</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>lee, sm</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>lim, yg</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>lee</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  year     name  height\n",
       "0    1  kim, js     170\n",
       "1    1  lee, sm     175\n",
       "2    2  lim, yg     180\n",
       "3    2      lee     170"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf.write.format('com.databricks.spark.csv').save(os.path.join('data','_myDf.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " C 드라이브의 볼륨에는 이름이 없습니다.\n",
      " 볼륨 일련 번호: 8EE8-038A\n",
      "\n",
      " C:\\Users\\qorgk\\code\\spark\\data\\_myDf.csv 디렉터리\n",
      "\n",
      "2019-10-08  오후 04:22    <DIR>          .\n",
      "2019-10-08  오후 04:22    <DIR>          ..\n",
      "2019-10-08  오후 04:22                12 .part-r-00000-aea5e80f-e6b4-4547-b5fb-bfcbbbbc6ead.csv.crc\n",
      "2019-10-08  오후 04:22                12 .part-r-00001-aea5e80f-e6b4-4547-b5fb-bfcbbbbc6ead.csv.crc\n",
      "2019-10-08  오후 04:22                 8 ._SUCCESS.crc\n",
      "2019-10-08  오후 04:22                32 part-r-00000-aea5e80f-e6b4-4547-b5fb-bfcbbbbc6ead.csv\n",
      "2019-10-08  오후 04:22                26 part-r-00001-aea5e80f-e6b4-4547-b5fb-bfcbbbbc6ead.csv\n",
      "2019-10-08  오후 04:22                 0 _SUCCESS\n",
      "               6개 파일                  90 바이트\n",
      "               2개 디렉터리  442,167,955,456 바이트 남음\n"
     ]
    }
   ],
   "source": [
    "!dir \"data/_myDf.csv/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf.toPandas().to_csv(os.path.join('data','myDf.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "web data ㅊㅓㄹㅣ\n",
    "1. HTML\n",
    "2. JSON\n",
    " - {k,v} k:column,v:value\n",
    "3. XML\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/ds_twitter_seoul_3.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_twitter_seoul_3.json\n",
    "{\"contributors\": null, \"truncated\": false, \"text\": \"RT @soompi: #SEVENTEEN’s Mingyu, Jin Se Yeon, And Leeteuk To MC For 2016 Super Seoul Dream Concert \\nhttps://t.co/1XRSaRBbE0 https://t.co/fi…\", \"is_quote_status\": false, \"in_reply_to_status_id\": null, \"id\": 801657325836763136, \"favorite_count\": 0, \"entities\": {\"symbols\": [], \"user_mentions\": [{\"id\": 17659206, \"indices\": [3, 10], \"id_str\": \"17659206\", \"screen_name\": \"soompi\", \"name\": \"Soompi\"}], \"hashtags\": [{\"indices\": [12, 22], \"text\": \"SEVENTEEN\"}], \"urls\": [{\"url\": \"https://t.co/1XRSaRBbE0\", \"indices\": [100, 123], \"expanded_url\": \"http://www.soompi.com/2016/11/20/seventeens-mingyu-jin-se-yeon-leeteuk-mc-dream-concert/\", \"display_url\": \"soompi.com/2016/11/20/sev…\"}]}, \"retweeted\": false, \"coordinates\": null, \"source\": \"<a href=\\\"http://twitter.com/download/android\\\" rel=\\\"nofollow\\\">Twitter for Android</a>\", \"in_reply_to_screen_name\": null, \"in_reply_to_user_id\": null, \"retweet_count\": 1487, \"id_str\": \"801657325836763136\", \"favorited\": false, \"retweeted_status\": {\"contributors\": null, \"truncated\": false, \"text\": \"#SEVENTEEN’s Mingyu, Jin Se Yeon, And Leeteuk To MC For 2016 Super Seoul Dream Concert \\nhttps://t.co/1XRSaRBbE0 https://t.co/fifXHpF8or\", \"is_quote_status\": false, \"in_reply_to_status_id\": null, \"id\": 800593781586132993, \"favorite_count\": 1649, \"entities\": {\"symbols\": [], \"user_mentions\": [], \"hashtags\": [{\"indices\": [0, 10], \"text\": \"SEVENTEEN\"}], \"urls\": [{\"url\": \"https://t.co/1XRSaRBbE0\", \"indices\": [88, 111], \"expanded_url\": \"http://www.soompi.com/2016/11/20/seventeens-mingyu-jin-se-yeon-leeteuk-mc-dream-concert/\", \"display_url\": \"soompi.com/2016/11/20/sev…\"}], \"media\": [{\"expanded_url\": \"https://twitter.com/soompi/status/800593781586132993/photo/1\", \"display_url\": \"pic.twitter.com/fifXHpF8or\", \"url\": \"https://t.co/fifXHpF8or\", \"media_url_https\": \"https://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\", \"id_str\": \"800593115165798400\", \"sizes\": {\"small\": {\"h\": 382, \"resize\": \"fit\", \"w\": 680}, \"large\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"medium\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"thumb\": {\"h\": 150, \"resize\": \"crop\", \"w\": 150}}, \"indices\": [112, 135], \"type\": \"photo\", \"id\": 800593115165798400, \"media_url\": \"http://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\"}]}, \"retweeted\": false, \"coordinates\": null, \"source\": \"<a href=\\\"https://about.twitter.com/products/tweetdeck\\\" rel=\\\"nofollow\\\">TweetDeck</a>\", \"in_reply_to_screen_name\": null, \"in_reply_to_user_id\": null, \"retweet_count\": 1487, \"id_str\": \"800593781586132993\", \"favorited\": false, \"user\": {\"follow_request_sent\": false, \"has_extended_profile\": true, \"profile_use_background_image\": true, \"default_profile_image\": false, \"id\": 17659206, \"profile_background_image_url_https\": \"https://pbs.twimg.com/profile_background_images/699864769/1cdde0a85f5c0a994ae1fb06d545a5ec.png\", \"verified\": true, \"translator_type\": \"none\", \"profile_text_color\": \"999999\", \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/792117259489583104/4khJk3zz_normal.jpg\", \"profile_sidebar_fill_color\": \"000000\", \"entities\": {\"url\": {\"urls\": [{\"url\": \"http://t.co/3evT80UlR9\", \"indices\": [0, 22], \"expanded_url\": \"http://www.soompi.com\", \"display_url\": \"soompi.com\"}]}, \"description\": {\"urls\": []}}, \"followers_count\": 987867, \"profile_sidebar_border_color\": \"000000\", \"id_str\": \"17659206\", \"profile_background_color\": \"1E1E1E\", \"listed_count\": 3982, \"is_translation_enabled\": true, \"utc_offset\": -28800, \"statuses_count\": 80038, \"description\": \"The original K-pop community. We take gifs, OTPs, and reporting on your bias' fashion choices seriously. But not rumors. Ain't nobody got time for that.\", \"friends_count\": 3532, \"location\": \"Worldwide\", \"profile_link_color\": \"31B6F4\", \"profile_image_url\": \"http://pbs.twimg.com/profile_images/792117259489583104/4khJk3zz_normal.jpg\", \"following\": false, \"geo_enabled\": false, \"profile_banner_url\": \"https://pbs.twimg.com/profile_banners/17659206/1478803767\", \"profile_background_image_url\": \"http://pbs.twimg.com/profile_background_images/699864769/1cdde0a85f5c0a994ae1fb06d545a5ec.png\", \"screen_name\": \"soompi\", \"lang\": \"en\", \"profile_background_tile\": true, \"favourites_count\": 1493, \"name\": \"Soompi\", \"notifications\": false, \"url\": \"http://t.co/3evT80UlR9\", \"created_at\": \"Wed Nov 26 20:48:27 +0000 2008\", \"contributors_enabled\": false, \"time_zone\": \"Pacific Time (US & Canada)\", \"protected\": false, \"default_profile\": false, \"is_translator\": false}, \"geo\": null, \"in_reply_to_user_id_str\": null, \"possibly_sensitive\": false, \"lang\": \"en\", \"created_at\": \"Mon Nov 21 06:56:46 +0000 2016\", \"in_reply_to_status_id_str\": null, \"place\": null, \"extended_entities\": {\"media\": [{\"expanded_url\": \"https://twitter.com/soompi/status/800593781586132993/photo/1\", \"display_url\": \"pic.twitter.com/fifXHpF8or\", \"url\": \"https://t.co/fifXHpF8or\", \"media_url_https\": \"https://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\", \"id_str\": \"800593115165798400\", \"sizes\": {\"small\": {\"h\": 382, \"resize\": \"fit\", \"w\": 680}, \"large\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"medium\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"thumb\": {\"h\": 150, \"resize\": \"crop\", \"w\": 150}}, \"indices\": [112, 135], \"type\": \"photo\", \"id\": 800593115165798400, \"media_url\": \"http://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\"}]}, \"metadata\": {\"iso_language_code\": \"en\", \"result_type\": \"recent\"}}, \"user\": {\"follow_request_sent\": false, \"has_extended_profile\": false, \"profile_use_background_image\": true, \"default_profile_image\": true, \"id\": 791090169818521600, \"profile_background_image_url_https\": null, \"verified\": false, \"translator_type\": \"none\", \"profile_text_color\": \"333333\", \"profile_image_url_https\": \"https://abs.twimg.com/sticky/default_profile_images/default_profile_6_normal.png\", \"profile_sidebar_fill_color\": \"DDEEF6\", \"entities\": {\"description\": {\"urls\": []}}, \"followers_count\": 0, \"profile_sidebar_border_color\": \"C0DEED\", \"id_str\": \"791090169818521600\", \"profile_background_color\": \"F5F8FA\", \"listed_count\": 0, \"is_translation_enabled\": false, \"utc_offset\": null, \"statuses_count\": 96, \"description\": \"\", \"friends_count\": 7, \"location\": \"\", \"profile_link_color\": \"1DA1F2\", \"profile_image_url\": \"http://abs.twimg.com/sticky/default_profile_images/default_profile_6_normal.png\", \"following\": false, \"geo_enabled\": false, \"profile_background_image_url\": null, \"screen_name\": \"enriquesanq\", \"lang\": \"es\", \"profile_background_tile\": false, \"favourites_count\": 161, \"name\": \"Enrique santos\", \"notifications\": false, \"url\": null, \"created_at\": \"Wed Oct 26 01:32:49 +0000 2016\", \"contributors_enabled\": false, \"time_zone\": null, \"protected\": false, \"default_profile\": true, \"is_translator\": false}, \"geo\": null, \"in_reply_to_user_id_str\": null, \"possibly_sensitive\": false, \"lang\": \"en\", \"created_at\": \"Thu Nov 24 05:22:55 +0000 2016\", \"in_reply_to_status_id_str\": null, \"place\": null, \"metadata\": {\"iso_language_code\": \"en\", \"result_type\": \"recent\"}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "_jfname=os.path.join('src','ds_twitter_seoul_3.json')\n",
    "with open(_jfname, 'rb') as f:\n",
    "    data = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open : ㅇㅣㄹㄱㅇㅡㄴ ㅍㅏㅇㅣㄹㅇㅡㄹ closed ㅎㅐㅈㅜㅈㅣ ㅇㅏㄴㅎㅇㅏㄷㅗ ㄱㅗㅐㄴㅊㅜㄴ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"contributors\": null, \"truncated\": false, \"text\": \"RT @soompi: #SEVENTEEN\\xe2\\x80\\x99s Mingyu, Jin Se Yeon, And Leeteuk To MC For 2016 Super Seoul Dream Concert \\\\nhttps://t.co/1XRSaRBbE0 https://t.co/fi\\xe2\\x80\\xa6\", \"is_quote_status\": false, \"in_reply_to_status_id\": null, \"id\": 801657325836763136, \"favorite_count\": 0, \"entities\": {\"symbols\": [], \"user_mentions\": [{\"id\": 17659206, \"indices\": [3, 10], \"id_str\": \"17659206\", \"screen_name\": \"soompi\", \"name\": \"Soompi\"}], \"hashtags\": [{\"indices\": [12, 22], \"text\": \"SEVENTEEN\"}], \"urls\": [{\"url\": \"https://t.co/1XRSaRBbE0\", \"indices\": [100, 123], \"expanded_url\": \"http://www.soompi.com/2016/11/20/seventeens-mingyu-jin-se-yeon-leeteuk-mc-dream-concert/\", \"display_url\": \"soompi.com/2016/11/20/sev\\xe2\\x80\\xa6\"}]}, \"retweeted\": false, \"coordinates\": null, \"source\": \"<a href=\\\\\"http://twitter.com/download/android\\\\\" rel=\\\\\"nofollow\\\\\">Twitter for Android</a>\", \"in_reply_to_screen_name\": null, \"in_reply_to_user_id\": null, \"retweet_count\": 1487, \"id_str\": \"801657325836763136\", \"favorited\": false, \"retweeted_status\": {\"contributors\": null, \"truncated\": false, \"text\": \"#SEVENTEEN\\xe2\\x80\\x99s Mingyu, Jin Se Yeon, And Leeteuk To MC For 2016 Super Seoul Dream Concert \\\\nhttps://t.co/1XRSaRBbE0 https://t.co/fifXHpF8or\", \"is_quote_status\": false, \"in_reply_to_status_id\": null, \"id\": 800593781586132993, \"favorite_count\": 1649, \"entities\": {\"symbols\": [], \"user_mentions\": [], \"hashtags\": [{\"indices\": [0, 10], \"text\": \"SEVENTEEN\"}], \"urls\": [{\"url\": \"https://t.co/1XRSaRBbE0\", \"indices\": [88, 111], \"expanded_url\": \"http://www.soompi.com/2016/11/20/seventeens-mingyu-jin-se-yeon-leeteuk-mc-dream-concert/\", \"display_url\": \"soompi.com/2016/11/20/sev\\xe2\\x80\\xa6\"}], \"media\": [{\"expanded_url\": \"https://twitter.com/soompi/status/800593781586132993/photo/1\", \"display_url\": \"pic.twitter.com/fifXHpF8or\", \"url\": \"https://t.co/fifXHpF8or\", \"media_url_https\": \"https://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\", \"id_str\": \"800593115165798400\", \"sizes\": {\"small\": {\"h\": 382, \"resize\": \"fit\", \"w\": 680}, \"large\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"medium\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"thumb\": {\"h\": 150, \"resize\": \"crop\", \"w\": 150}}, \"indices\": [112, 135], \"type\": \"photo\", \"id\": 800593115165798400, \"media_url\": \"http://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\"}]}, \"retweeted\": false, \"coordinates\": null, \"source\": \"<a href=\\\\\"https://about.twitter.com/products/tweetdeck\\\\\" rel=\\\\\"nofollow\\\\\">TweetDeck</a>\", \"in_reply_to_screen_name\": null, \"in_reply_to_user_id\": null, \"retweet_count\": 1487, \"id_str\": \"800593781586132993\", \"favorited\": false, \"user\": {\"follow_request_sent\": false, \"has_extended_profile\": true, \"profile_use_background_image\": true, \"default_profile_image\": false, \"id\": 17659206, \"profile_background_image_url_https\": \"https://pbs.twimg.com/profile_background_images/699864769/1cdde0a85f5c0a994ae1fb06d545a5ec.png\", \"verified\": true, \"translator_type\": \"none\", \"profile_text_color\": \"999999\", \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/792117259489583104/4khJk3zz_normal.jpg\", \"profile_sidebar_fill_color\": \"000000\", \"entities\": {\"url\": {\"urls\": [{\"url\": \"http://t.co/3evT80UlR9\", \"indices\": [0, 22], \"expanded_url\": \"http://www.soompi.com\", \"display_url\": \"soompi.com\"}]}, \"description\": {\"urls\": []}}, \"followers_count\": 987867, \"profile_sidebar_border_color\": \"000000\", \"id_str\": \"17659206\", \"profile_background_color\": \"1E1E1E\", \"listed_count\": 3982, \"is_translation_enabled\": true, \"utc_offset\": -28800, \"statuses_count\": 80038, \"description\": \"The original K-pop community. We take gifs, OTPs, and reporting on your bias\\' fashion choices seriously. But not rumors. Ain\\'t nobody got time for that.\", \"friends_count\": 3532, \"location\": \"Worldwide\", \"profile_link_color\": \"31B6F4\", \"profile_image_url\": \"http://pbs.twimg.com/profile_images/792117259489583104/4khJk3zz_normal.jpg\", \"following\": false, \"geo_enabled\": false, \"profile_banner_url\": \"https://pbs.twimg.com/profile_banners/17659206/1478803767\", \"profile_background_image_url\": \"http://pbs.twimg.com/profile_background_images/699864769/1cdde0a85f5c0a994ae1fb06d545a5ec.png\", \"screen_name\": \"soompi\", \"lang\": \"en\", \"profile_background_tile\": true, \"favourites_count\": 1493, \"name\": \"Soompi\", \"notifications\": false, \"url\": \"http://t.co/3evT80UlR9\", \"created_at\": \"Wed Nov 26 20:48:27 +0000 2008\", \"contributors_enabled\": false, \"time_zone\": \"Pacific Time (US & Canada)\", \"protected\": false, \"default_profile\": false, \"is_translator\": false}, \"geo\": null, \"in_reply_to_user_id_str\": null, \"possibly_sensitive\": false, \"lang\": \"en\", \"created_at\": \"Mon Nov 21 06:56:46 +0000 2016\", \"in_reply_to_status_id_str\": null, \"place\": null, \"extended_entities\": {\"media\": [{\"expanded_url\": \"https://twitter.com/soompi/status/800593781586132993/photo/1\", \"display_url\": \"pic.twitter.com/fifXHpF8or\", \"url\": \"https://t.co/fifXHpF8or\", \"media_url_https\": \"https://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\", \"id_str\": \"800593115165798400\", \"sizes\": {\"small\": {\"h\": 382, \"resize\": \"fit\", \"w\": 680}, \"large\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"medium\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"thumb\": {\"h\": 150, \"resize\": \"crop\", \"w\": 150}}, \"indices\": [112, 135], \"type\": \"photo\", \"id\": 800593115165798400, \"media_url\": \"http://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\"}]}, \"metadata\": {\"iso_language_code\": \"en\", \"result_type\": \"recent\"}}, \"user\": {\"follow_request_sent\": false, \"has_extended_profile\": false, \"profile_use_background_image\": true, \"default_profile_image\": true, \"id\": 791090169818521600, \"profile_background_image_url_https\": null, \"verified\": false, \"translator_type\": \"none\", \"profile_text_color\": \"333333\", \"profile_image_url_https\": \"https://abs.twimg.com/sticky/default_profile_images/default_profile_6_normal.png\", \"profile_sidebar_fill_color\": \"DDEEF6\", \"entities\": {\"description\": {\"urls\": []}}, \"followers_count\": 0, \"profile_sidebar_border_color\": \"C0DEED\", \"id_str\": \"791090169818521600\", \"profile_background_color\": \"F5F8FA\", \"listed_count\": 0, \"is_translation_enabled\": false, \"utc_offset\": null, \"statuses_count\": 96, \"description\": \"\", \"friends_count\": 7, \"location\": \"\", \"profile_link_color\": \"1DA1F2\", \"profile_image_url\": \"http://abs.twimg.com/sticky/default_profile_images/default_profile_6_normal.png\", \"following\": false, \"geo_enabled\": false, \"profile_background_image_url\": null, \"screen_name\": \"enriquesanq\", \"lang\": \"es\", \"profile_background_tile\": false, \"favourites_count\": 161, \"name\": \"Enrique santos\", \"notifications\": false, \"url\": null, \"created_at\": \"Wed Oct 26 01:32:49 +0000 2016\", \"contributors_enabled\": false, \"time_zone\": null, \"protected\": false, \"default_profile\": true, \"is_translator\": false}, \"geo\": null, \"in_reply_to_user_id_str\": null, \"possibly_sensitive\": false, \"lang\": \"en\", \"created_at\": \"Thu Nov 24 05:22:55 +0000 2016\", \"in_reply_to_status_id_str\": null, \"place\": null, \"metadata\": {\"iso_language_code\": \"en\", \"result_type\": \"recent\"}}']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#jsonㅇㅡㄴ stringㅇㅡㄹㅗ ㅈㅓㅈㅏㅇㄷㅗㅣㄱㅗ ㅅㅏㅇㅛㅇㅎㅏㄹ ㄸㅐ ㅍㅏㅅㅣㅇㅎㅐㅈㅜㅇㅓㅇㅑㅎㅏㄴㄷㅏ.\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = map(lambda x: x.rstrip(), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A,B,C'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\",\".join([\"A\", \"B\", \"C\"]) # A,B,C ㅇㅕㄴㄱㅕㄹㅈㅏ,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A+B+C'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"+\".join([\"A\", \"B\", \"C\"]) #ㅇㅕㄴㄱㅕㄹㅈㅏ +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json_str = \"[\" + ','.join(data) + \"]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6908"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_df = pd.read_json(data_json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contributors                 0\n",
      "coordinates                  0\n",
      "created_at                   1\n",
      "entities                     1\n",
      "favorite_count               1\n",
      "favorited                    1\n",
      "geo                          0\n",
      "id                           1\n",
      "id_str                       1\n",
      "in_reply_to_screen_name      0\n",
      "in_reply_to_status_id        0\n",
      "in_reply_to_status_id_str    0\n",
      "in_reply_to_user_id          0\n",
      "in_reply_to_user_id_str      0\n",
      "is_quote_status              1\n",
      "lang                         1\n",
      "metadata                     1\n",
      "place                        0\n",
      "possibly_sensitive           1\n",
      "retweet_count                1\n",
      "retweeted                    1\n",
      "retweeted_status             1\n",
      "source                       1\n",
      "text                         1\n",
      "truncated                    1\n",
      "user                         1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print data_df.count() #count ㅎㅐㅇㅇㅡㅣ ㄱㅐㅅㅜ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    801657325836763136\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df['id'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "cfile= os.path.join(os.environ[\"SPARK_HOME\"],\\\n",
    "           \"examples/src/main/resources/people.txt\")\n",
    "lines = spark.sparkContext.textFile(cfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "people = parts.map(lambda p: Row(name=p[0], age=int(p[1].strip())))\n",
    "\n",
    "_myDf = spark.createDataFrame(people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(age=29, name=u'Michael'),\n",
       " Row(age=30, name=u'Andy'),\n",
       " Row(age=19, name=u'Justin')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_myDf.printSchema()\n",
    "_myDf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Df = spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('data/publicBike.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|              �뿩����|�뿩�Ǽ�|\n",
      "+--------------------+-----+\n",
      "|2018-01-01 00:00:...| 4950|\n",
      "|2018-01-02 00:00:...| 7136|\n",
      "|2018-01-03 00:00:...| 7156|\n",
      "|2018-01-04 00:00:...| 7102|\n",
      "|2018-01-05 00:00:...| 7705|\n",
      "|2018-01-06 00:00:...| 5681|\n",
      "|2018-01-07 00:00:...| 5220|\n",
      "|2018-01-08 00:00:...| 6309|\n",
      "|2018-01-09 00:00:...| 5988|\n",
      "|2018-01-10 00:00:...| 4476|\n",
      "|2018-01-11 00:00:...| 4337|\n",
      "|2018-01-12 00:00:...| 4401|\n",
      "|2018-01-13 00:00:...| 3756|\n",
      "|2018-01-14 00:00:...| 4675|\n",
      "|2018-01-15 00:00:...| 6993|\n",
      "|2018-01-16 00:00:...| 7421|\n",
      "|2018-01-17 00:00:...| 6990|\n",
      "|2018-01-18 00:00:...| 7054|\n",
      "|2018-01-19 00:00:...| 8329|\n",
      "|2018-01-20 00:00:...| 6148|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- �뿩����: timestamp (nullable = true)\n",
      " |-- �뿩�Ǽ�: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Df.show()\n",
    "Df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Df=Df.withColumnRenamed('�뿩����','date')\n",
    "Df=Df.withColumnRenamed('�뿩�Ǽ�','count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                date|count|\n",
      "+--------------------+-----+\n",
      "|2018-01-01 00:00:...| 4950|\n",
      "|2018-01-02 00:00:...| 7136|\n",
      "|2018-01-03 00:00:...| 7156|\n",
      "|2018-01-04 00:00:...| 7102|\n",
      "|2018-01-05 00:00:...| 7705|\n",
      "|2018-01-06 00:00:...| 5681|\n",
      "|2018-01-07 00:00:...| 5220|\n",
      "|2018-01-08 00:00:...| 6309|\n",
      "|2018-01-09 00:00:...| 5988|\n",
      "|2018-01-10 00:00:...| 4476|\n",
      "|2018-01-11 00:00:...| 4337|\n",
      "|2018-01-12 00:00:...| 4401|\n",
      "|2018-01-13 00:00:...| 3756|\n",
      "|2018-01-14 00:00:...| 4675|\n",
      "|2018-01-15 00:00:...| 6993|\n",
      "|2018-01-16 00:00:...| 7421|\n",
      "|2018-01-17 00:00:...| 6990|\n",
      "|2018-01-18 00:00:...| 7054|\n",
      "|2018-01-19 00:00:...| 8329|\n",
      "|2018-01-20 00:00:...| 6148|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Df.show()\n",
    "Df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_col = pyspark.sql.functions.split(Df['date'], '-')\n",
    "Df = Df.withColumn('year', split_col.getItem(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----+\n",
      "|                date|count|year|\n",
      "+--------------------+-----+----+\n",
      "|2018-01-01 00:00:...| 4950|2018|\n",
      "|2018-01-02 00:00:...| 7136|2018|\n",
      "|2018-01-03 00:00:...| 7156|2018|\n",
      "|2018-01-04 00:00:...| 7102|2018|\n",
      "|2018-01-05 00:00:...| 7705|2018|\n",
      "|2018-01-06 00:00:...| 5681|2018|\n",
      "|2018-01-07 00:00:...| 5220|2018|\n",
      "|2018-01-08 00:00:...| 6309|2018|\n",
      "|2018-01-09 00:00:...| 5988|2018|\n",
      "|2018-01-10 00:00:...| 4476|2018|\n",
      "|2018-01-11 00:00:...| 4337|2018|\n",
      "|2018-01-12 00:00:...| 4401|2018|\n",
      "|2018-01-13 00:00:...| 3756|2018|\n",
      "|2018-01-14 00:00:...| 4675|2018|\n",
      "|2018-01-15 00:00:...| 6993|2018|\n",
      "|2018-01-16 00:00:...| 7421|2018|\n",
      "|2018-01-17 00:00:...| 6990|2018|\n",
      "|2018-01-18 00:00:...| 7054|2018|\n",
      "|2018-01-19 00:00:...| 8329|2018|\n",
      "|2018-01-20 00:00:...| 6148|2018|\n",
      "+--------------------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Df.show()\n",
    "Df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|year|sum(count)|\n",
      "+----+----------+\n",
      "|2019|   1871935|\n",
      "|2018|  10124874|\n",
      "+----+----------+\n",
      "\n",
      "+----+-----+\n",
      "|year|count|\n",
      "+----+-----+\n",
      "|2019|   90|\n",
      "|2018|  365|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Df.groupBy('year').sum('count').show()          \n",
    "Df.groupBy('year').count().show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.658985, 4.285136])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([float(x) for x in '1.658985\t4.285136'.split('\\t')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.658985, 4.285136])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([float(x) for x in '1.658985 4.285136'.split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/ds_spark_heightweight.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark_heightweight.txt\n",
    "1\t65.78\t112.99\n",
    "2\t71.52\t136.49\n",
    "3\t69.40\t153.03\n",
    "4\t68.22\t142.34\n",
    "5\t67.79\t144.30\n",
    "6\t68.70\t123.30\n",
    "7\t69.80\t141.49\n",
    "8\t70.01\t136.46\n",
    "9\t67.90\t112.37\n",
    "10\t66.78\t120.67\n",
    "11\t66.49\t127.45\n",
    "12\t67.62\t114.14\n",
    "13\t68.30\t125.61\n",
    "14\t67.12\t122.46\n",
    "15\t68.28\t116.09\n",
    "16\t71.09\t140.00\n",
    "17\t66.46\t129.50\n",
    "18\t68.65\t142.97\n",
    "19\t71.23\t137.90\n",
    "20\t67.13\t124.04\n",
    "21\t67.83\t141.28\n",
    "22\t68.88\t143.54\n",
    "23\t63.48\t97.90\n",
    "24\t68.42\t129.50\n",
    "25\t67.63\t141.85\n",
    "26\t67.21\t129.72\n",
    "27\t70.84\t142.42\n",
    "28\t67.49\t131.55\n",
    "29\t66.53\t108.33\n",
    "30\t65.44\t113.89\n",
    "31\t69.52\t103.30\n",
    "32\t65.81\t120.75\n",
    "33\t67.82\t125.79\n",
    "34\t70.60\t136.22\n",
    "35\t71.80\t140.10\n",
    "36\t69.21\t128.75\n",
    "37\t66.80\t141.80\n",
    "38\t67.66\t121.23\n",
    "39\t67.81\t131.35\n",
    "40\t64.05\t106.71\n",
    "41\t68.57\t124.36\n",
    "42\t65.18\t124.86\n",
    "43\t69.66\t139.67\n",
    "44\t67.97\t137.37\n",
    "45\t65.98\t106.45\n",
    "46\t68.67\t128.76\n",
    "47\t66.88\t145.68\n",
    "48\t67.70\t116.82\n",
    "49\t69.82\t143.62\n",
    "50\t69.09\t134.93\n",
    "51\t69.91\t147.02\n",
    "52\t67.33\t126.33\n",
    "53\t70.27\t125.48\n",
    "54\t69.10\t115.71\n",
    "55\t65.38\t123.49\n",
    "56\t70.18\t147.89\n",
    "57\t70.41\t155.90\n",
    "58\t66.54\t128.07\n",
    "59\t66.36\t119.37\n",
    "60\t67.54\t133.81\n",
    "61\t66.50\t128.73\n",
    "62\t69.00\t137.55\n",
    "63\t68.30\t129.76\n",
    "64\t67.01\t128.82\n",
    "65\t70.81\t135.32\n",
    "66\t68.22\t109.61\n",
    "67\t69.06\t142.47\n",
    "68\t67.73\t132.75\n",
    "69\t67.22\t103.53\n",
    "70\t67.37\t124.73\n",
    "71\t65.27\t129.31\n",
    "72\t70.84\t134.02\n",
    "73\t69.92\t140.40\n",
    "74\t64.29\t102.84\n",
    "75\t68.25\t128.52\n",
    "76\t66.36\t120.30\n",
    "77\t68.36\t138.60\n",
    "78\t65.48\t132.96\n",
    "79\t69.72\t115.62\n",
    "80\t67.73\t122.52\n",
    "81\t68.64\t134.63\n",
    "82\t66.78\t121.90\n",
    "83\t70.05\t155.38\n",
    "84\t66.28\t128.94\n",
    "85\t69.20\t129.10\n",
    "86\t69.13\t139.47\n",
    "87\t67.36\t140.89\n",
    "88\t70.09\t131.59\n",
    "89\t70.18\t121.12\n",
    "90\t68.23\t131.51\n",
    "91\t68.13\t136.55\n",
    "92\t70.24\t141.49\n",
    "93\t71.49\t140.61\n",
    "94\t69.20\t112.14\n",
    "95\t70.06\t133.46\n",
    "96\t70.56\t131.80\n",
    "97\t66.29\t120.03\n",
    "98\t63.43\t123.10\n",
    "99\t66.77\t128.14\n",
    "100\t68.89\t115.48\n",
    "101\t64.87\t102.09\n",
    "102\t67.09\t130.35\n",
    "103\t68.35\t134.18\n",
    "104\t65.61\t98.64\n",
    "105\t67.76\t114.56\n",
    "106\t68.02\t123.49\n",
    "107\t67.66\t123.05\n",
    "108\t66.31\t126.48\n",
    "109\t69.44\t128.42\n",
    "110\t63.84\t127.19\n",
    "111\t67.72\t122.06\n",
    "112\t70.05\t127.61\n",
    "113\t70.19\t131.64\n",
    "114\t65.95\t111.90\n",
    "115\t70.01\t122.04\n",
    "116\t68.61\t128.55\n",
    "117\t68.81\t132.68\n",
    "118\t69.76\t136.06\n",
    "119\t65.46\t115.94\n",
    "120\t68.83\t136.90\n",
    "121\t65.80\t119.88\n",
    "122\t67.21\t109.01\n",
    "123\t69.42\t128.27\n",
    "124\t68.94\t135.29\n",
    "125\t67.94\t106.86\n",
    "126\t65.63\t123.29\n",
    "127\t66.50\t109.51\n",
    "128\t67.93\t119.31\n",
    "129\t68.89\t140.24\n",
    "130\t70.24\t133.98\n",
    "131\t68.27\t132.58\n",
    "132\t71.23\t130.70\n",
    "133\t69.10\t115.56\n",
    "134\t64.40\t123.79\n",
    "135\t71.10\t128.14\n",
    "136\t68.22\t135.96\n",
    "137\t65.92\t116.63\n",
    "138\t67.44\t126.82\n",
    "139\t73.90\t151.39\n",
    "140\t69.98\t130.40\n",
    "141\t69.52\t136.21\n",
    "142\t65.18\t113.40\n",
    "143\t68.01\t125.33\n",
    "144\t68.34\t127.58\n",
    "145\t65.18\t107.16\n",
    "146\t68.26\t116.46\n",
    "147\t68.57\t133.84\n",
    "148\t64.50\t112.89\n",
    "149\t68.71\t130.76\n",
    "150\t68.89\t137.76\n",
    "151\t69.54\t125.40\n",
    "152\t67.40\t138.47\n",
    "153\t66.48\t120.82\n",
    "154\t66.01\t140.15\n",
    "155\t72.44\t136.74\n",
    "156\t64.13\t106.11\n",
    "157\t70.98\t158.96\n",
    "158\t67.50\t108.79\n",
    "159\t72.02\t138.78\n",
    "160\t65.31\t115.91\n",
    "161\t67.08\t146.29\n",
    "162\t64.39\t109.88\n",
    "163\t69.37\t139.05\n",
    "164\t68.38\t119.90\n",
    "165\t65.31\t128.31\n",
    "166\t67.14\t127.24\n",
    "167\t68.39\t115.23\n",
    "168\t66.29\t124.80\n",
    "169\t67.19\t126.95\n",
    "170\t65.99\t111.27\n",
    "171\t69.43\t122.61\n",
    "172\t67.97\t124.21\n",
    "173\t67.76\t124.65\n",
    "174\t65.28\t119.52\n",
    "175\t73.83\t139.30\n",
    "176\t66.81\t104.83\n",
    "177\t66.89\t123.04\n",
    "178\t65.74\t118.89\n",
    "179\t65.98\t121.49\n",
    "180\t66.58\t119.25\n",
    "181\t67.11\t135.02\n",
    "182\t65.87\t116.23\n",
    "183\t66.78\t109.17\n",
    "184\t68.74\t124.22\n",
    "185\t66.23\t141.16\n",
    "186\t65.96\t129.15\n",
    "187\t68.58\t127.87\n",
    "188\t66.59\t120.92\n",
    "189\t66.97\t127.65\n",
    "190\t68.08\t101.47\n",
    "191\t70.19\t144.99\n",
    "192\t65.52\t110.95\n",
    "193\t67.46\t132.86\n",
    "194\t67.41\t146.34\n",
    "195\t69.66\t145.59\n",
    "196\t65.80\t120.84\n",
    "197\t66.11\t115.78\n",
    "198\t68.24\t128.30\n",
    "199\t68.02\t127.47\n",
    "200\t71.39\t127.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load data/ds_spark_heightweight.txt\n",
    "1\t65.78\t112.99\n",
    "2\t71.52\t136.49\n",
    "3\t69.40\t153.03\n",
    "4\t68.22\t142.34\n",
    "5\t67.79\t144.30\n",
    "6\t68.70\t123.30\n",
    "7\t69.80\t141.49\n",
    "8\t70.01\t136.46\n",
    "9\t67.90\t112.37\n",
    "10\t66.78\t120.67\n",
    "11\t66.49\t127.45\n",
    "12\t67.62\t114.14\n",
    "13\t68.30\t125.61\n",
    "14\t67.12\t122.46\n",
    "15\t68.28\t116.09\n",
    "16\t71.09\t140.00\n",
    "17\t66.46\t129.50\n",
    "18\t68.65\t142.97\n",
    "19\t71.23\t137.90\n",
    "20\t67.13\t124.04\n",
    "21\t67.83\t141.28\n",
    "22\t68.88\t143.54\n",
    "23\t63.48\t97.90\n",
    "24\t68.42\t129.50\n",
    "25\t67.63\t141.85\n",
    "26\t67.21\t129.72\n",
    "27\t70.84\t142.42\n",
    "28\t67.49\t131.55\n",
    "29\t66.53\t108.33\n",
    "30\t65.44\t113.89\n",
    "31\t69.52\t103.30\n",
    "32\t65.81\t120.75\n",
    "33\t67.82\t125.79\n",
    "34\t70.60\t136.22\n",
    "35\t71.80\t140.10\n",
    "36\t69.21\t128.75\n",
    "37\t66.80\t141.80\n",
    "38\t67.66\t121.23\n",
    "39\t67.81\t131.35\n",
    "40\t64.05\t106.71\n",
    "41\t68.57\t124.36\n",
    "42\t65.18\t124.86\n",
    "43\t69.66\t139.67\n",
    "44\t67.97\t137.37\n",
    "45\t65.98\t106.45\n",
    "46\t68.67\t128.76\n",
    "47\t66.88\t145.68\n",
    "48\t67.70\t116.82\n",
    "49\t69.82\t143.62\n",
    "50\t69.09\t134.93\n",
    "51\t69.91\t147.02\n",
    "52\t67.33\t126.33\n",
    "53\t70.27\t125.48\n",
    "54\t69.10\t115.71\n",
    "55\t65.38\t123.49\n",
    "56\t70.18\t147.89\n",
    "57\t70.41\t155.90\n",
    "58\t66.54\t128.07\n",
    "59\t66.36\t119.37\n",
    "60\t67.54\t133.81\n",
    "61\t66.50\t128.73\n",
    "62\t69.00\t137.55\n",
    "63\t68.30\t129.76\n",
    "64\t67.01\t128.82\n",
    "65\t70.81\t135.32\n",
    "66\t68.22\t109.61\n",
    "67\t69.06\t142.47\n",
    "68\t67.73\t132.75\n",
    "69\t67.22\t103.53\n",
    "70\t67.37\t124.73\n",
    "71\t65.27\t129.31\n",
    "72\t70.84\t134.02\n",
    "73\t69.92\t140.40\n",
    "74\t64.29\t102.84\n",
    "75\t68.25\t128.52\n",
    "76\t66.36\t120.30\n",
    "77\t68.36\t138.60\n",
    "78\t65.48\t132.96\n",
    "79\t69.72\t115.62\n",
    "80\t67.73\t122.52\n",
    "81\t68.64\t134.63\n",
    "82\t66.78\t121.90\n",
    "83\t70.05\t155.38\n",
    "84\t66.28\t128.94\n",
    "85\t69.20\t129.10\n",
    "86\t69.13\t139.47\n",
    "87\t67.36\t140.89\n",
    "88\t70.09\t131.59\n",
    "89\t70.18\t121.12\n",
    "90\t68.23\t131.51\n",
    "91\t68.13\t136.55\n",
    "92\t70.24\t141.49\n",
    "93\t71.49\t140.61\n",
    "94\t69.20\t112.14\n",
    "95\t70.06\t133.46\n",
    "96\t70.56\t131.80\n",
    "97\t66.29\t120.03\n",
    "98\t63.43\t123.10\n",
    "99\t66.77\t128.14\n",
    "100\t68.89\t115.48\n",
    "101\t64.87\t102.09\n",
    "102\t67.09\t130.35\n",
    "103\t68.35\t134.18\n",
    "104\t65.61\t98.64\n",
    "105\t67.76\t114.56\n",
    "106\t68.02\t123.49\n",
    "107\t67.66\t123.05\n",
    "108\t66.31\t126.48\n",
    "109\t69.44\t128.42\n",
    "110\t63.84\t127.19\n",
    "111\t67.72\t122.06\n",
    "112\t70.05\t127.61\n",
    "113\t70.19\t131.64\n",
    "114\t65.95\t111.90\n",
    "115\t70.01\t122.04\n",
    "116\t68.61\t128.55\n",
    "117\t68.81\t132.68\n",
    "118\t69.76\t136.06\n",
    "119\t65.46\t115.94\n",
    "120\t68.83\t136.90\n",
    "121\t65.80\t119.88\n",
    "122\t67.21\t109.01\n",
    "123\t69.42\t128.27\n",
    "124\t68.94\t135.29\n",
    "125\t67.94\t106.86\n",
    "126\t65.63\t123.29\n",
    "127\t66.50\t109.51\n",
    "128\t67.93\t119.31\n",
    "129\t68.89\t140.24\n",
    "130\t70.24\t133.98\n",
    "131\t68.27\t132.58\n",
    "132\t71.23\t130.70\n",
    "133\t69.10\t115.56\n",
    "134\t64.40\t123.79\n",
    "135\t71.10\t128.14\n",
    "136\t68.22\t135.96\n",
    "137\t65.92\t116.63\n",
    "138\t67.44\t126.82\n",
    "139\t73.90\t151.39\n",
    "140\t69.98\t130.40\n",
    "141\t69.52\t136.21\n",
    "142\t65.18\t113.40\n",
    "143\t68.01\t125.33\n",
    "144\t68.34\t127.58\n",
    "145\t65.18\t107.16\n",
    "146\t68.26\t116.46\n",
    "147\t68.57\t133.84\n",
    "148\t64.50\t112.89\n",
    "149\t68.71\t130.76\n",
    "150\t68.89\t137.76\n",
    "151\t69.54\t125.40\n",
    "152\t67.40\t138.47\n",
    "153\t66.48\t120.82\n",
    "154\t66.01\t140.15\n",
    "155\t72.44\t136.74\n",
    "156\t64.13\t106.11\n",
    "157\t70.98\t158.96\n",
    "158\t67.50\t108.79\n",
    "159\t72.02\t138.78\n",
    "160\t65.31\t115.91\n",
    "161\t67.08\t146.29\n",
    "162\t64.39\t109.88\n",
    "163\t69.37\t139.05\n",
    "164\t68.38\t119.90\n",
    "165\t65.31\t128.31\n",
    "166\t67.14\t127.24\n",
    "167\t68.39\t115.23\n",
    "168\t66.29\t124.80\n",
    "169\t67.19\t126.95\n",
    "170\t65.99\t111.27\n",
    "171\t69.43\t122.61\n",
    "172\t67.97\t124.21\n",
    "173\t67.76\t124.65\n",
    "174\t65.28\t119.52\n",
    "175\t73.83\t139.30\n",
    "176\t66.81\t104.83\n",
    "177\t66.89\t123.04\n",
    "178\t65.74\t118.89\n",
    "179\t65.98\t121.49\n",
    "180\t66.58\t119.25\n",
    "181\t67.11\t135.02\n",
    "182\t65.87\t116.23\n",
    "183\t66.78\t109.17\n",
    "184\t68.74\t124.22\n",
    "185\t66.23\t141.16\n",
    "186\t65.96\t129.15\n",
    "187\t68.58\t127.87\n",
    "188\t66.59\t120.92\n",
    "189\t66.97\t127.65\n",
    "190\t68.08\t101.47\n",
    "191\t70.19\t144.99\n",
    "192\t65.52\t110.95\n",
    "193\t67.46\t132.86\n",
    "194\t67.41\t146.34\n",
    "195\t69.66\t145.59\n",
    "196\t65.80\t120.84\n",
    "197\t66.11\t115.78\n",
    "198\t68.24\t128.30\n",
    "199\t68.02\t127.47\n",
    "200\t71.39\t127.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "rdd=spark.sparkContext.textFile(os.path.join('data','ds_spark_heightweight.txt'))\n",
    "\n",
    "tRdd=rdd.map(lambda x:x.split('\\t'))\n",
    "tDf=spark.createDataFrame(tRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| _1|   _2|    _3|\n",
      "+---+-----+------+\n",
      "|  1|65.78|112.99|\n",
      "+---+-----+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDf.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1=u'1', _2=u'65.78', _3=u'112.99')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tDf=tDf.withColumn(\"id\",tDf['_1'].cast(\"integer\")).drop('_1')\n",
    "tDf=tDf.withColumn(\"height\",tDf['_2'].cast(\"double\")).drop('_2')\n",
    "tDf=tDf.withColumn(\"weight\",tDf['_3'].cast(\"double\")).drop('_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, height=65.78, weight=112.99)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| id|height|weight|\n",
      "+---+------+------+\n",
      "|  1| 65.78|112.99|\n",
      "+---+------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDf.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 65.78, 112.99]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tRdd=rdd.map(lambda line:[float(x) for x in line.split('\\t')])\n",
    "tRdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDf=spark.createDataFrame(tRdd,[\"id\",\"weight\",\"height\"])\n",
    "tDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[112.99 136.49 153.03 142.34 144.3 ]\n",
      "[65.78 71.52 69.4  68.22 67.79]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "_heightRdd=tDf.rdd.map(lambda fields:fields[1]).collect()\n",
    "_weightRdd=tDf.rdd.map(lambda fields:fields[2]).collect()\n",
    "print np.array(_weightRdd)[:5]\n",
    "print np.array(_heightRdd)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.array(_weightRdd), np.array(_heightRdd),'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래프 그리기 중요하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "jfile= os.path.join(os.environ[\"SPARK_HOME\"],\\\n",
    "           \"examples/src/main/resources/people.json\")\n",
    "\n",
    "_myDf= spark.read.json(jfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf.filter(_myDf['age'] > 21).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "#list 형식으로 작성된 데이터\n",
    "r=requests.get(\"https://raw.githubusercontent.com/jokecamp/FootballData/master/World%20Cups/all-world-cup-players.json\")\n",
    "wc=r.json()\n",
    "# 한번 저장해두고 사용하는 것이 좋음 사이트에 한번식 들렸다가 오는 것이라서 시간이 오래걸림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'> <type 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print type(wc), type(wc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'Club': u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada',\n",
       " u'ClubCountry': u'Argentina',\n",
       " u'Competition': u'World Cup',\n",
       " u'DateOfBirth': u'1905-5-5',\n",
       " u'FullName': u'\\xc3ngel Bossio',\n",
       " u'IsCaptain': False,\n",
       " u'Number': u'',\n",
       " u'Position': u'GK',\n",
       " u'Team': u'Argentina',\n",
       " u'Year': 1930}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "web data는 숫자가 없음 모두 문자!\n",
    "문자열을 json으로 변경할 시 parsing이 필수다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcDF=spark.createDataFrame(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Club=u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada', ClubCountry=u'Argentina', Competition=u'World Cup', DateOfBirth=u'1905-5-5', FullName=u'\\xc3ngel Bossio', IsCaptain=False, Number=u'', Position=u'GK', Team=u'Argentina', Year=1930)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcDF.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-----------+-----------+------------+---------+------+--------+---------+----+\n",
      "|                Club|ClubCountry|Competition|DateOfBirth|    FullName|IsCaptain|Number|Position|     Team|Year|\n",
      "+--------------------+-----------+-----------+-----------+------------+---------+------+--------+---------+----+\n",
      "|Club AtlÃ©tico Ta...|  Argentina|  World Cup|   1905-5-5|Ãngel Bossio|    false|      |      GK|Argentina|1930|\n",
      "+--------------------+-----------+-----------+-----------+------------+---------+------+--------+---------+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDF.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcRdd=spark.sparkContext.parallelize(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'Club': u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada',\n",
       "  u'ClubCountry': u'Argentina',\n",
       "  u'Competition': u'World Cup',\n",
       "  u'DateOfBirth': u'1905-5-5',\n",
       "  u'FullName': u'\\xc3ngel Bossio',\n",
       "  u'IsCaptain': False,\n",
       "  u'Number': u'',\n",
       "  u'Position': u'GK',\n",
       "  u'Team': u'Argentina',\n",
       "  u'Year': 1930}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcRdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "wcSchema=StructType([\n",
    "    StructField(\"Club\", StringType(), True),\n",
    "    StructField(\"ClubCountry\", StringType(), True),\n",
    "    StructField(\"Competition\", StringType(), True),\n",
    "    StructField(\"DateOfBirth\", DateType(), True),\n",
    "    StructField(\"FullName\", StringType(), True),\n",
    "    StructField(\"IsCaptain\", BooleanType(), True),\n",
    "    StructField(\"Number\", IntegerType(), True),\n",
    "    StructField(\"Position\", StringType(), True),\n",
    "    StructField(\"Team\", StringType(), True),\n",
    "    StructField(\"Year\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcDF=spark.createDataFrame(wcRdd,wcSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: date (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: integer (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "_wcDF=spark.createDataFrame(wcRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_wcDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Club=u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada', ClubCountry=u'Argentina', Competition=u'World Cup', DateOfBirth=u'1905-5-5', FullName=u'\\xc3ngel Bossio', IsCaptain=False, Number=u'', Position=u'GK', Team=u'Argentina', Year=1930)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_wcDF.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.sql.execution.python.EvaluatePython.takeAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 34.0 failed 1 times, most recent failure: Lost task 0.0 in stage 34.0 (TID 231, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\qorgk\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in main\n  File \"C:\\Users\\qorgk\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 167, in process\n  File \"C:\\Users\\qorgk\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\qorgk\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\session.py\", line 505, in prepare\n  File \"C:\\Users\\qorgk\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 1317, in _verify_type\n    raise TypeError(\"StructType can not accept object %r in type %s\" % (obj, type(obj)))\nTypeError: StructType can not accept object {u'Club': u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada', u'ClubCountry': u'Argentina', u'Team': u'Argentina', u'Number': u'', u'Competition': u'World Cup', u'DateOfBirth': u'1905-5-5', u'Year': 1930, u'Position': u'GK', u'FullName': u'\\xc3ngel Bossio', u'IsCaptain': False} in type <type 'dict'>\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)\r\n\tat java.base/java.lang.Thread.run(Thread.java:844)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:347)\r\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$takeAndServe$1.apply$mcI$sp(EvaluatePython.scala:41)\r\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$takeAndServe$1.apply(EvaluatePython.scala:39)\r\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$takeAndServe$1.apply(EvaluatePython.scala:39)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\r\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2532)\r\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.takeAndServe(EvaluatePython.scala:39)\r\n\tat org.apache.spark.sql.execution.python.EvaluatePython.takeAndServe(EvaluatePython.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\r\n\tat java.base/java.lang.Thread.run(Thread.java:844)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\qorgk\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in main\n  File \"C:\\Users\\qorgk\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 167, in process\n  File \"C:\\Users\\qorgk\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\qorgk\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\session.py\", line 505, in prepare\n  File \"C:\\Users\\qorgk\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 1317, in _verify_type\n    raise TypeError(\"StructType can not accept object %r in type %s\" % (obj, type(obj)))\nTypeError: StructType can not accept object {u'Club': u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada', u'ClubCountry': u'Argentina', u'Team': u'Argentina', u'Number': u'', u'Competition': u'World Cup', u'DateOfBirth': u'1905-5-5', u'Year': 1930, u'Position': u'GK', u'FullName': u'\\xc3ngel Bossio', u'IsCaptain': False} in type <type 'dict'>\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-a62de550ee83>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwcDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\qorgk\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m    348\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m             port = self._sc._jvm.org.apache.spark.sql.execution.python.EvaluatePython.takeAndServe(\n\u001b[1;32m--> 350\u001b[1;33m                 self._jdf, num)\n\u001b[0m\u001b[0;32m    351\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\qorgk\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.1-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 933\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\qorgk\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\qorgk\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.1-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    310\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    311\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    313\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.sql.execution.python.EvaluatePython.takeAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 34.0 failed 1 times, most recent failure: Lost task 0.0 in stage 34.0 (TID 231, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\qorgk\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in main\n  File \"C:\\Users\\qorgk\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 167, in process\n  File \"C:\\Users\\qorgk\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\qorgk\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\session.py\", line 505, in prepare\n  File \"C:\\Users\\qorgk\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 1317, in _verify_type\n    raise TypeError(\"StructType can not accept object %r in type %s\" % (obj, type(obj)))\nTypeError: StructType can not accept object {u'Club': u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada', u'ClubCountry': u'Argentina', u'Team': u'Argentina', u'Number': u'', u'Competition': u'World Cup', u'DateOfBirth': u'1905-5-5', u'Year': 1930, u'Position': u'GK', u'FullName': u'\\xc3ngel Bossio', u'IsCaptain': False} in type <type 'dict'>\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)\r\n\tat java.base/java.lang.Thread.run(Thread.java:844)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:347)\r\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$takeAndServe$1.apply$mcI$sp(EvaluatePython.scala:41)\r\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$takeAndServe$1.apply(EvaluatePython.scala:39)\r\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$takeAndServe$1.apply(EvaluatePython.scala:39)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\r\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2532)\r\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.takeAndServe(EvaluatePython.scala:39)\r\n\tat org.apache.spark.sql.execution.python.EvaluatePython.takeAndServe(EvaluatePython.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\r\n\tat java.base/java.lang.Thread.run(Thread.java:844)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\qorgk\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in main\n  File \"C:\\Users\\qorgk\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 167, in process\n  File \"C:\\Users\\qorgk\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\qorgk\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\session.py\", line 505, in prepare\n  File \"C:\\Users\\qorgk\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 1317, in _verify_type\n    raise TypeError(\"StructType can not accept object %r in type %s\" % (obj, type(obj)))\nTypeError: StructType can not accept object {u'Club': u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada', u'ClubCountry': u'Argentina', u'Team': u'Argentina', u'Number': u'', u'Competition': u'World Cup', u'DateOfBirth': u'1905-5-5', u'Year': 1930, u'Position': u'GK', u'FullName': u'\\xc3ngel Bossio', u'IsCaptain': False} in type <type 'dict'>\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "wcDF.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991-11-25 00:00:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print datetime.strptime(\"11/25/1991\", '%m/%d/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "toDate = udf(lambda x: datetime.strptime(x, '%m/%d/%Y'), DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "_wcDF = _wcDF.withColumn('date1', toDate(_wcDF['DateOfBirth']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- date1: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_wcDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "_wcDF=_wcDF.withColumn('date2', to_date(_wcDF['DateOfBirth']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "\n",
    "_wcDF=_wcDF.withColumn('date3', _wcDF['DateOfBirth'].cast(DateType()))\n",
    "_wcDF=_wcDF.withColumn('NumberInt', _wcDF['Number'].cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- date1: date (nullable = true)\n",
      " |-- date3: date (nullable = true)\n",
      " |-- NumberInt: integer (nullable = true)\n",
      " |-- date2: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_wcDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "_wcDF=_wcDF.drop('date1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Club=u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada', ClubNation=u'Argentina', Competition=u'World Cup', DateOfBirth=u'1905-5-5', FullName=u'\\xc3ngel Bossio', IsCaptain=False, Number=u'', Position=u'GK', Team=u'Argentina', Year=1930, date3=datetime.date(1905, 5, 5), NumberInt=None, date2=datetime.date(1905, 5, 5))]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_wcDF.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|ClubCountry|count|\n",
      "+-----------+-----+\n",
      "|   England |    4|\n",
      "|   Paraguay|   93|\n",
      "|     Russia|   51|\n",
      "|        POL|   11|\n",
      "|        BRA|   27|\n",
      "|    Senegal|    1|\n",
      "|     Sweden|  154|\n",
      "|   Colombia|    1|\n",
      "|        FRA|  155|\n",
      "|        ALG|    8|\n",
      "|   England |    1|\n",
      "|       RUS |    1|\n",
      "|     Turkey|   65|\n",
      "|      Zaire|   22|\n",
      "|       Iraq|   22|\n",
      "|    Germany|  206|\n",
      "|        RSA|   16|\n",
      "|        ITA|  224|\n",
      "|        UKR|   38|\n",
      "|        GHA|    8|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_wcDF.groupBy(_wcDF.ClubCountry).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+----+----+----+----+\n",
      "|ClubCountry|    |  DF|  FW|  GK|  MF|\n",
      "+-----------+----+----+----+----+----+\n",
      "|   England |null|null|   2|null|   2|\n",
      "|   Paraguay|null|  26|  37|  10|  20|\n",
      "|     Russia|null|  20|  11|   4|  16|\n",
      "|        POL|null|   2|   2|   3|   4|\n",
      "|        BRA|null|   7|   5|   4|  11|\n",
      "|    Senegal|null|null|null|   1|null|\n",
      "|     Sweden|null|  40|  47|  25|  42|\n",
      "|   Colombia|null|null|   1|null|null|\n",
      "|        ALG|null|   2|null|   6|null|\n",
      "|        FRA|null|  46|  41|  18|  50|\n",
      "|   England |null|null|null|null|   1|\n",
      "|       RUS |null|null|null|   1|null|\n",
      "|     Turkey|null|  20|  13|  12|  20|\n",
      "|      Zaire|null|   6|   5|   3|   8|\n",
      "|       Iraq|null|   6|   4|   3|   9|\n",
      "|    Germany|null|  64|  51|  16|  75|\n",
      "|        RSA|null|   5|   2|   3|   6|\n",
      "|        UKR|null|  13|   7|   4|  14|\n",
      "|        ITA|null|  74|  42|  19|  89|\n",
      "|        CMR|null|   1|   1|   1|null|\n",
      "+-----------+----+----+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_wcDF.groupBy('ClubCountry').pivot('Position').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf = spark.read.format('com.databricks.spark.csv')\\\n",
    "    .options(header='true', inferschema='true').load('data/myDf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      "\n",
      "+---+----+-------+------+\n",
      "|_c0|year|   name|height|\n",
      "+---+----+-------+------+\n",
      "|  0|   1|kim, js|   170|\n",
      "|  1|   1|lee, sm|   175|\n",
      "|  2|   2|lim, yg|   180|\n",
      "|  3|   2|    lee|   170|\n",
      "+---+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "_wcDF=_wcDF.withColumnRenamed('ClubCountry','ClubNation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubNation: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- date1: date (nullable = true)\n",
      " |-- date3: date (nullable = true)\n",
      " |-- NumberInt: integer (nullable = true)\n",
      " |-- date2: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_wcDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<name>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<name>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name=u'kim, js'),\n",
       " Row(name=u'lee, sm'),\n",
       " Row(name=u'lim, yg'),\n",
       " Row(name=u'lee')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_name=myDf.select('name')\n",
    "_name.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'kim', u' js']\n"
     ]
    }
   ],
   "source": [
    "r=Row(name=u'kim, js')\n",
    "rd=r.asDict()\n",
    "print rd.values()[0].split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|height|\n",
      "+-------+------+\n",
      "|kim, js|   170|\n",
      "|    lee|   170|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#where 행 select 열\n",
    "myDf.where(myDf['height'] < 175)\\\n",
    "    .select(myDf['name'], myDf['height']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+---------+-----------+\n",
      "|year|max(_c0)|max(year)|max(height)|\n",
      "+----+--------+---------+-----------+\n",
      "|   1|       1|        1|        175|\n",
      "|   2|       3|        2|        180|\n",
      "+----+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.groupby(myDf['year']).max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "##udf(lambda파라미터, return, return type)\n",
    "toDoublefunc = udf(lambda x: float(x),DoubleType())\n",
    "myDf = myDf.withColumn(\"heightD\",toDoublefunc(myDf.height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      " |-- heightD: double (nullable = true)\n",
      "\n",
      "+---+----+-------+------+-------+\n",
      "|_c0|year|   name|height|heightD|\n",
      "+---+----+-------+------+-------+\n",
      "|  0|   1|kim, js|   170|  170.0|\n",
      "|  1|   1|lee, sm|   175|  175.0|\n",
      "|  2|   2|lim, yg|   180|  180.0|\n",
      "|  3|   2|    lee|   170|  170.0|\n",
      "+---+----+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+-------+---------+\n",
      "|_c0|year|   name|height|heightD|nameUpper|\n",
      "+---+----+-------+------+-------+---------+\n",
      "|  0|   1|kim, js|   170|  170.0|  KIM, JS|\n",
      "|  1|   1|lee, sm|   175|  175.0|  LEE, SM|\n",
      "|  2|   2|lim, yg|   180|  180.0|  LIM, YG|\n",
      "|  3|   2|    lee|   170|  170.0|      LEE|\n",
      "+---+----+-------+------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    " \n",
    "def uppercase(s):\n",
    "    return s.upper()\n",
    "\n",
    "upperUdf = udf(uppercase, StringType())\n",
    "myDf = myDf.withColumn(\"nameUpper\", upperUdf(myDf['name']))\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+-------+---------+----------+\n",
      "|_c0|year|   name|height|heightD|nameUpper|nameUpper2|\n",
      "+---+----+-------+------+-------+---------+----------+\n",
      "|  0|   1|kim, js|   170|  170.0|  KIM, JS|   KIM, JS|\n",
      "|  1|   1|lee, sm|   175|  175.0|  LEE, SM|   LEE, SM|\n",
      "|  2|   2|lim, yg|   180|  180.0|  LIM, YG|   LIM, YG|\n",
      "|  3|   2|    lee|   170|  170.0|      LEE|       LEE|\n",
      "+---+----+-------+------+-------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "upperUdf = udf(lambda x : x.upper(), StringType())\n",
    "myDf = myDf.withColumn(\"nameUpper2\", upperUdf(myDf['name']))\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+-------+---------+----------+----------+\n",
      "|_c0|year|   name|height|heightD|nameUpper|nameUpper2|height>175|\n",
      "+---+----+-------+------+-------+---------+----------+----------+\n",
      "|  0|   1|kim, js|   170|  170.0|  KIM, JS|   KIM, JS|   shorter|\n",
      "|  1|   1|lee, sm|   175|  175.0|  LEE, SM|   LEE, SM|    taller|\n",
      "|  2|   2|lim, yg|   180|  180.0|  LIM, YG|   LIM, YG|    taller|\n",
      "|  3|   2|    lee|   170|  170.0|      LEE|       LEE|   shorter|\n",
      "+---+----+-------+------+-------+---------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "height_udf = udf(lambda height: \"taller\" if height >=175 else \"shorter\", StringType())\n",
    "heightDf=myDf.withColumn(\"height>175\", height_udf(myDf.heightD))\n",
    "heightDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf = myDf.drop('nameUpper2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+-------+---------+\n",
      "|_c0|year|   name|height|heightD|nameUpper|\n",
      "+---+----+-------+------+-------+---------+\n",
      "|  0|   1|kim, js|   170|  170.0|  KIM, JS|\n",
      "|  1|   1|lee, sm|   175|  175.0|  LEE, SM|\n",
      "|  2|   2|lim, yg|   180|  180.0|  LIM, YG|\n",
      "|  3|   2|    lee|   170|  170.0|      LEE|\n",
      "+---+----+-------+------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|summary|               _c0|              year|            height|           heightD|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|  count|                 4|                 4|                 4|                 4|\n",
      "|   mean|               1.5|               1.5|            173.75|            173.75|\n",
      "| stddev|1.2909944487358056|0.5773502691896257|4.7871355387816905|4.7871355387816905|\n",
      "|    min|                 0|                 1|               170|             170.0|\n",
      "|    max|                 3|                 2|               180|             180.0|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubNation: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- date3: date (nullable = true)\n",
      " |-- NumberInt: integer (nullable = true)\n",
      " |-- date2: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_wcDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "_wcDF = _wcDF.drop('date2')\n",
    "_wcDF = _wcDF.drop('date3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+----+\n",
      "|                Club|     Team|Year|\n",
      "+--------------------+---------+----+\n",
      "|Club AtlÃ©tico Ta...|Argentina|1930|\n",
      "|Quilmes AtlÃ©tico...|Argentina|1930|\n",
      "|          Boca Junio|Argentina|1930|\n",
      "|Central Norte TucumÃ|Argentina|1930|\n",
      "|Club Atletico Est...|Argentina|1930|\n",
      "|Racing Club de Av...|Argentina|1930|\n",
      "|     Sportivo Barrac|Argentina|1930|\n",
      "|          Boca Junio|Argentina|1930|\n",
      "|Estudiantes de La...|Argentina|1930|\n",
      "|Club AtlÃ©tico Sa...|Argentina|1930|\n",
      "|          Boca Junio|Argentina|1930|\n",
      "|Club Atletico Est...|Argentina|1930|\n",
      "|Racing Club de Av...|Argentina|1930|\n",
      "|Racing Club de Av...|Argentina|1930|\n",
      "|Club Sportivo Bue...|Argentina|1930|\n",
      "|Club AtlÃ©tico La...|Argentina|1930|\n",
      "|Estudiantes de La...|Argentina|1930|\n",
      "|Club AtlÃ©tico La...|Argentina|1930|\n",
      "|Club AtlÃ©tico Hu...|Argentina|1930|\n",
      "|          Boca Junio|Argentina|1930|\n",
      "+--------------------+---------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_wcDF.createOrReplaceTempView(\"wc\")\n",
    "spark.sql(\"select Club,Team,Year from wc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+---------+----+\n",
      "|    FullName|                Club|     Team|Year|\n",
      "+------------+--------------------+---------+----+\n",
      "|Ãngel Bossio|Club AtlÃ©tico Ta...|Argentina|1930|\n",
      "+------------+--------------------+---------+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcPlayers=spark.sql(\"select FullName,Club,Team,Year from wc\")\n",
    "wcPlayers.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name=u'wc', database=None, description=None, tableType=u'TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full name: Ãngel Bossio\n",
      "Full name: Juan Botasso\n",
      "Full name: Roberto Cherro\n",
      "Full name: Alberto Chividini\n",
      "Full name: \n"
     ]
    }
   ],
   "source": [
    "namesRdd=wcPlayers.rdd.map(lambda x: \"Full name: \"+x[0])\n",
    "#for문으로 보지 않으면 유니코드가 깨져서 나옴\n",
    "for e in namesRdd.take(5):\n",
    "    print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------------------+\n",
      "|bucketId|items                       |\n",
      "+--------+----------------------------+\n",
      "|1       |[orange, apple, pineapple]  |\n",
      "|2       |[watermelon, apple, bananas]|\n",
      "+--------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bucketDf=spark.createDataFrame([[1,[\"orange\", \"apple\", \"pineapple\"]],\n",
    "                                [2,[\"watermelon\",\"apple\",\"bananas\"]]],\n",
    "                               [\"bucketId\",\"items\"])\n",
    "bucketDf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|bucketId|      item|\n",
      "+--------+----------+\n",
      "|       1|    orange|\n",
      "|       1|     apple|\n",
      "|       1| pineapple|\n",
      "|       2|watermelon|\n",
      "|       2|     apple|\n",
      "|       2|   bananas|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "bDf=bucketDf.select(bucketDf.bucketId,explode(bucketDf.items).alias('item'))\n",
    "bDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|      item|itemId|\n",
      "+----------+------+\n",
      "|    orange|    F1|\n",
      "|     apple|    F2|\n",
      "| pineapple|    F3|\n",
      "|watermelon|    F4|\n",
      "|   bananas|    F5|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fDf=spark.createDataFrame([[\"orange\", \"F1\"],\n",
    "                            [\"apple\", \"F2\"],\n",
    "                            [\"pineapple\",\"F3\"],\n",
    "                            [\"watermelon\",\"F4\"],\n",
    "                            [\"bananas\",\"F5\"]],\n",
    "                            [\"item\",\"itemId\"])\n",
    "fDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+\n",
      "|itemId|      item|bucketId|\n",
      "+------+----------+--------+\n",
      "|    F5|   bananas|       2|\n",
      "|    F1|    orange|       1|\n",
      "|    F2|     apple|       1|\n",
      "|    F2|     apple|       2|\n",
      "|    F3| pineapple|       1|\n",
      "|    F4|watermelon|       2|\n",
      "+------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinDf=fDf.join(bDf, fDf.item==bDf.item, \"inner\")\n",
    "joinDf.select(fDf.itemId,fDf.item,bDf.bucketId).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+-------+---------+\n",
      "|_c0|year|   name|height|heightD|nameUpper|\n",
      "+---+----+-------+------+-------+---------+\n",
      "|  0|   1|kim, js|   170|  170.0|  KIM, JS|\n",
      "|  1|   1|lee, sm|   175|  175.0|  LEE, SM|\n",
      "|  2|   2|lim, yg|   180|  180.0|  LIM, YG|\n",
      "|  3|   2|    lee|   170|  170.0|      LEE|\n",
      "+---+----+-------+------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|how tall|\n",
      "+--------+\n",
      "|    <175|\n",
      "|    <175|\n",
      "|    >175|\n",
      "|    <175|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "_myDf=myDf.select(when(myDf['heightD'] >175.0, \">175\")\\\n",
    "            .otherwise(\"<175\").alias(\"how tall\"))\n",
    "_myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+\n",
      "|year|avg(heightD)|\n",
      "+----+------------+\n",
      "|   1|       172.5|\n",
      "|   2|       175.0|\n",
      "+----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.groupBy('year').agg({\"heightD\":\"avg\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "a=myDf.agg(F.min(myDf.heightD),F.max(myDf.heightD),F.avg(myDf.heightD),F.sum(myDf.heightD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------+------------+\n",
      "|min(heightD)|max(heightD)|avg(heightD)|sum(heightD)|\n",
      "+------------+------------+------------+------------+\n",
      "|       170.0|       180.0|      173.75|       695.0|\n",
      "+------------+------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
