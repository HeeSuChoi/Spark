{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"]=os.path.join(os.path.expanduser(\"~\"),'spark-2.0.0-bin-hadoop2.7')\n",
    "os.environ[\"PYLIB\"]=os.path.join(os.environ[\"SPARK_HOME\"],'python','lib')\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.1-src.zip'))\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'pyspark.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(i)? (<ipython-input-2-b7c99d3d530a>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-b7c99d3d530a>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    print i\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(i)?\n"
     ]
    }
   ],
   "source": [
    "os.path.join(os.path.expanduser(\"~\"),'spark-2.0.0-bin-hadoop2.7')\n",
    "for i in sys.path:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "# \\는 연결의 의미 한줄로 쓰지 않으려는\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'file:${system:user.dir}/spark-warehouse'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.sql.warehouse.dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "pyspark-shell\n",
      "local[*]\n",
      "192.168.0.2\n"
     ]
    }
   ],
   "source": [
    "print spark.version\n",
    "print spark.conf.get('spark.app.name')\n",
    "print spark.conf.get('spark.master')\n",
    "print spark.conf.get('spark.driver.host')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x71c63c8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'file:${system:user.dir}/spark-warehouse'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.sql.warehouse.dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "myList=[1,2,3,4,5,6,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:475"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize(myList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRdd1 = spark.sparkContext.parallelize(myList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(myRdd1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd1.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\qorgk\\\\spark-2.0.0-bin-hadoop2.7\\\\python\\\\lib\\\\pyspark.zip',\n",
       " 'C:\\\\Users\\\\qorgk\\\\spark-2.0.0-bin-hadoop2.7\\\\python\\\\lib\\\\py4j-0.10.1-src.zip',\n",
       " 'C:\\\\Users\\\\qorgk\\\\code\\\\spark',\n",
       " 'C:\\\\Users\\\\qorgk\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\python37.zip',\n",
       " 'C:\\\\Users\\\\qorgk\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\DLLs',\n",
       " 'C:\\\\Users\\\\qorgk\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\lib',\n",
       " 'C:\\\\Users\\\\qorgk\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3',\n",
       " '',\n",
       " 'C:\\\\Users\\\\qorgk\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\lib\\\\site-packages',\n",
       " 'C:\\\\Users\\\\qorgk\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\lib\\\\site-packages\\\\win32',\n",
       " 'C:\\\\Users\\\\qorgk\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib',\n",
       " 'C:\\\\Users\\\\qorgk\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\lib\\\\site-packages\\\\Pythonwin',\n",
       " 'C:\\\\Users\\\\qorgk\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\lib\\\\site-packages\\\\IPython\\\\extensions',\n",
       " 'C:\\\\Users\\\\qorgk\\\\.ipython']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파일에서 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/ds_spark_wiki.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark_wiki.txt\n",
    "Wikipedia\n",
    "Apache Spark is an open source cluster computing framework.\n",
    "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
    "Apache Spark Apache Spark Apache Spark Apache Spark\n",
    "아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크\n",
    "Originally developed at the University of California, Berkeley's AMPLab,\n",
    "the Spark codebase was later donated to the Apache Software Foundation,\n",
    "which has maintained it since.\n",
    "Spark provides an interface for programming entire clusters with\n",
    "implicit data parallelism and fault-tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data\\ds_spark_wiki.txt MapPartitionsRDD[4] at textFile at NativeMethodAccessorImpl.java:-2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRdd2=spark.sparkContext.textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Wikipedia'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "u'java.net.URISyntaxException: Relative path in absolute URI: file:C:/Users/qorgk/code/spark/spark-warehouse'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-65d8f91099ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmyDf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ds_spark_wiki.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mmyDf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\qorgk\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mtext\u001b[1;34m(self, paths)\u001b[0m\n\u001b[0;32m    280\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m             \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpaths\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\qorgk\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.1-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 933\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\qorgk\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: u'java.net.URISyntaxException: Relative path in absolute URI: file:C:/Users/qorgk/code/spark/spark-warehouse'"
     ]
    }
   ],
   "source": [
    "myDf=spark.read.text(os.path.join(\"data\", \"ds_spark_wiki.txt\"))\n",
    "print myDf.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./data/ds_spark_2cols.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data/ds_spark_2cols.csv\n",
    "35, 2\n",
    "40, 27\n",
    "12, 38\n",
    "15, 31\n",
    "21, 1\n",
    "14, 19\n",
    "46, 1\n",
    "10, 34\n",
    "28, 3\n",
    "48, 1\n",
    "16, 2\n",
    "30, 3\n",
    "32, 2\n",
    "48, 1\n",
    "31, 2\n",
    "22, 1\n",
    "12, 3\n",
    "39, 29\n",
    "19, 37\n",
    "25, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'35, 2', u'40, 27', u'12, 38', u'15, 31', u'21, 1']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd4 = spark.sparkContext.textFile(os.path.join(\"data\",\"ds_spark_2cols.csv\"))\n",
    "myRdd4.take(5) # take로 text를 가져오면 그 타입이 list이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n"
     ]
    }
   ],
   "source": [
    "myList=myRdd4.take(5)\n",
    "print type(myList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- '35,2'로 연결된 리스트를 각각의 요소로 나눠주는 작업\n",
    "- '35','2'로 \n",
    "- map이라는 함수가 그 역할을 해줌 람다는 함수 이름이 없고 리턴값이 없다.\n",
    "- 라인의 분기점을 ,로 한다는 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'35', u' 2'],\n",
       " [u'40', u' 27'],\n",
       " [u'12', u' 38'],\n",
       " [u'15', u' 31'],\n",
       " [u'21', u' 1']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd5 = myRdd4.map(lambda line: line.split(','))\n",
    "myRdd5.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- flatMap은 2차원 요소를 1차원 요소로 바꿔주는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102.56, 97.7, 99.14, 100.03999999999999]\n"
     ]
    }
   ],
   "source": [
    "celsius = [39.2, 36.5, 37.3, 37.8]\n",
    "def c2f(c):\n",
    "    f=list()\n",
    "    for i in c:\n",
    "        _f=(float(9)/5)*i + 32\n",
    "        f.append(_f)\n",
    "    return f\n",
    "\n",
    "print c2f(celsius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102.56, 97.7, 99.14, 100.03999999999999]\n"
     ]
    }
   ],
   "source": [
    "#python의 map함수. spark아님\n",
    "celsius = [39.2, 36.5, 37.3, 37.8]\n",
    "def c2f(c):\n",
    "    return (float(9)/5)*c + 32\n",
    "\n",
    "#for문을 사용하지 않고 각각의 요소에 대해 함수를 적용시키는 함수\n",
    "f=map(c2f, celsius)\n",
    "print f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x*2\n",
    "y=f(1)\n",
    "print y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "#lambda 함수는 인자를 받고 return과 함수의 이름이 없다. 즉, 계속 사용하는 경우 사용하지 않는다.\n",
    "y=lambda x:x*2\n",
    "print y(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[102.56, 97.7, 99.14, 100.03999999999999]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(lambda c:(float(9)/5)*c + 32, celsius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['H'], ['e'], ['l'], ['l'], ['o'], [], ['W'], ['o'], ['r'], ['l'], ['d']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Hello World\"\n",
    "map(lambda x:x.split(),sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hello', 'World'], ['Good', 'Morining']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#결과값이 2차원 데이터 text 처리할 시 2차원 이상의 데이터의 경우 flatMap을 사용한다\n",
    "sentence = [\"Hello World\", \"Good Morining\"]\n",
    "map(lambda x:x.split(),sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[19] at RDD at PythonRDD.scala:48\n"
     ]
    }
   ],
   "source": [
    "nRdd = spark.sparkContext.parallelize([1, 2, 3, 4])\n",
    "squared = nRdd.map(lambda x: x * x)\n",
    "print squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16]\n"
     ]
    }
   ],
   "source": [
    "print squared.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 3, 5, 13, 21, 55]\n"
     ]
    }
   ],
   "source": [
    "fib = [0,1,1,2,3,5,8,13,21,34,55]\n",
    "result = filter(lambda x: x % 2, fib)\n",
    "print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many lines having 'Spark':  4\n"
     ]
    }
   ],
   "source": [
    "myRdd_spark=myRdd2.filter(lambda line: \"Spark\" in line)\n",
    "print \"How many lines having 'Spark': \",myRdd_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n"
     ]
    }
   ],
   "source": [
    "myRdd_unicode = myRdd2.filter(lambda line: u\"스파크\" in line)\n",
    "print myRdd_unicode.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stopword\n",
    "- 사용하지 않는 언어, 불용어\n",
    "- 처리해봤자 의미가 별로 없는 단어 즉, 단어 그 자체로는 의미가 없는 단어들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['is','am','are','the','for','a', 'an', 'at']\n",
    "myRdd_stop = myRdd2.flatMap(lambda x:x.split()).filter(lambda x: x not in stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia Apache Spark open source cluster computing framework. 아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다. Apache Spark Apache Spark Apache Spark Apache Spark 아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크 Originally developed University of California, Berkeley's AMPLab, Spark codebase was later donated to Apache Software Foundation, which has maintained it since. Spark provides interface programming entire clusters with implicit data parallelism and fault-tolerance.\n"
     ]
    }
   ],
   "source": [
    "for words in myRdd_stop.collect():\n",
    "    print words,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduce()\n",
    "- 다른 방식의 더하기\n",
    "- 앞 두 수를 x,y라고 하면 두수를 더하고 나온 값을 다시 x로 그 뒤의 값을 y로 설정하여\n",
    "- 더함을 반복한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(lambda x, y: x+y, range(1,101))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd100 = spark.sparkContext.parallelize(range(1,101))\n",
    "myRdd100.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum:  10\n",
      "min:  1\n",
      "max:  4\n",
      "standard deviation: 1.118033988749895\n",
      "variance:  1.25\n"
     ]
    }
   ],
   "source": [
    "print \"sum: \",nRdd.sum()\n",
    "print \"min: \",nRdd.min()\n",
    "print \"max: \", nRdd.max()\n",
    "print \"standard deviation:\", nRdd.stdev()\n",
    "print \"variance: \", nRdd.variance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### foreach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.parallelize([1, 2, 3, 4, 5]).foreach(lambda x: x + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize([1, 2, 3, 4, 5]).map(lambda x: x + 1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt에서 하면 결과를 볼 수 있다\n",
    "def f(x): print(x)\n",
    "spark.sparkContext.parallelize([1, 2, 3, 4, 5]).foreach(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRdd2=spark.sparkContext.textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=myRdd2.map(lambda x:x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mySplit(x):\n",
    "    return x.split(\" \")\n",
    "\n",
    "#함수를 사용하면 lambda를 사용하지 않는다\n",
    "sentences2=myRdd2.map(mySplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'Wikipedia'],\n",
       " [u'Apache',\n",
       "  u'Spark',\n",
       "  u'is',\n",
       "  u'an',\n",
       "  u'open',\n",
       "  u'source',\n",
       "  u'cluster',\n",
       "  u'computing',\n",
       "  u'framework.'],\n",
       " [u'\\uc544\\ud30c\\uce58',\n",
       "  u'\\uc2a4\\ud30c\\ud06c\\ub294',\n",
       "  u'\\uc624\\ud508',\n",
       "  u'\\uc18c\\uc2a4',\n",
       "  u'\\ud074\\ub7ec\\uc2a4\\ud130',\n",
       "  u'\\ucef4\\ud4e8\\ud305',\n",
       "  u'\\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.']]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'Wikipedia'],\n",
       " [u'Apache',\n",
       "  u'Spark',\n",
       "  u'is',\n",
       "  u'an',\n",
       "  u'open',\n",
       "  u'source',\n",
       "  u'cluster',\n",
       "  u'computing',\n",
       "  u'framework.'],\n",
       " [u'\\uc544\\ud30c\\uce58',\n",
       "  u'\\uc2a4\\ud30c\\ud06c\\ub294',\n",
       "  u'\\uc624\\ud508',\n",
       "  u'\\uc18c\\uc2a4',\n",
       "  u'\\ud074\\ub7ec\\uc2a4\\ud130',\n",
       "  u'\\ucef4\\ud4e8\\ud305',\n",
       "  u'\\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.']]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.collect()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'Wikipedia'],\n",
       " [u'Apache',\n",
       "  u'Spark',\n",
       "  u'is',\n",
       "  u'an',\n",
       "  u'open',\n",
       "  u'source',\n",
       "  u'cluster',\n",
       "  u'computing',\n",
       "  u'framework.'],\n",
       " [u'\\uc544\\ud30c\\uce58',\n",
       "  u'\\uc2a4\\ud30c\\ud06c\\ub294',\n",
       "  u'\\uc624\\ud508',\n",
       "  u'\\uc18c\\uc2a4',\n",
       "  u'\\ud074\\ub7ec\\uc2a4\\ud130',\n",
       "  u'\\ucef4\\ud4e8\\ud305',\n",
       "  u'\\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.'],\n",
       " [u'Apache',\n",
       "  u'Spark',\n",
       "  u'Apache',\n",
       "  u'Spark',\n",
       "  u'Apache',\n",
       "  u'Spark',\n",
       "  u'Apache',\n",
       "  u'Spark'],\n",
       " [u'\\uc544\\ud30c\\uce58',\n",
       "  u'\\uc2a4\\ud30c\\ud06c',\n",
       "  u'\\uc544\\ud30c\\uce58',\n",
       "  u'\\uc2a4\\ud30c\\ud06c',\n",
       "  u'\\uc544\\ud30c\\uce58',\n",
       "  u'\\uc2a4\\ud30c\\ud06c',\n",
       "  u'\\uc544\\ud30c\\uce58',\n",
       "  u'\\uc2a4\\ud30c\\ud06c'],\n",
       " [u'Originally',\n",
       "  u'developed',\n",
       "  u'at',\n",
       "  u'the',\n",
       "  u'University',\n",
       "  u'of',\n",
       "  u'California,',\n",
       "  u\"Berkeley's\",\n",
       "  u'AMPLab,'],\n",
       " [u'the',\n",
       "  u'Spark',\n",
       "  u'codebase',\n",
       "  u'was',\n",
       "  u'later',\n",
       "  u'donated',\n",
       "  u'to',\n",
       "  u'the',\n",
       "  u'Apache',\n",
       "  u'Software',\n",
       "  u'Foundation,'],\n",
       " [u'which', u'has', u'maintained', u'it', u'since.'],\n",
       " [u'Spark',\n",
       "  u'provides',\n",
       "  u'an',\n",
       "  u'interface',\n",
       "  u'for',\n",
       "  u'programming',\n",
       "  u'entire',\n",
       "  u'clusters',\n",
       "  u'with'],\n",
       " [u'implicit', u'data', u'parallelism', u'and', u'fault-tolerance.']]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia \n",
      "-----\n",
      "Apache Spark is an open source cluster computing framework. \n",
      "-----\n",
      "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다. \n",
      "-----\n",
      "Apache Spark Apache Spark Apache Spark Apache Spark \n",
      "-----\n",
      "아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크 \n",
      "-----\n",
      "Originally developed at the University of California, Berkeley's AMPLab, \n",
      "-----\n",
      "the Spark codebase was later donated to the Apache Software Foundation, \n",
      "-----\n",
      "which has maintained it since. \n",
      "-----\n",
      "Spark provides an interface for programming entire clusters with \n",
      "-----\n",
      "implicit data parallelism and fault-tolerance. \n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for line in sentences.collect():\n",
    "    for word in line:\n",
    "        print word,\n",
    "    print \"\\n-----\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"Apache Spark is an open source cluster computing framework\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 59, 32, 51, 31, 72, 71, 30, 64, 46]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2.map(lambda s:len(s)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "myList=[\"this is\",\"a line\"]\n",
    "_rdd=spark.sparkContext.parallelize(myList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'is'], ['a', 'line']]\n"
     ]
    }
   ],
   "source": [
    "wordsRdd=_rdd.map(lambda x:x.split())\n",
    "print wordsRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is', 'AA line']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RDD는 수정이 불가능하다. 여기서 바뀐것은 결과값이지 RDD가 아니다.\n",
    "repRdd=_rdd.map(lambda x:x.replace(\"a\",\"AA\"))\n",
    "repRdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n"
     ]
    }
   ],
   "source": [
    "upper2list=wordsRdd\\\n",
    "    .map(lambda x: [i.upper() for i in x])\\\n",
    "    .collect()\n",
    "print type(upper2list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2]\n"
     ]
    }
   ],
   "source": [
    "wordsLength = wordsRdd\\\n",
    "    .map(len)\\\n",
    "    .collect()\n",
    "print wordsLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.parallelize(upper2list).saveAsTextFile(\"data/ds_spark_wiki_out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u\"['THIS', 'IS']\", u\"['A', 'LINE']\"]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rdd=spark.sparkContext.textFile(\"data/ds_spark_wiki_out\")\n",
    "_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Wikipedia',\n",
       " u'Apache Spark is an open source cluster computing framework.',\n",
       " u'\\uc544\\ud30c\\uce58 \\uc2a4\\ud30c\\ud06c\\ub294 \\uc624\\ud508 \\uc18c\\uc2a4 \\ud074\\ub7ec\\uc2a4\\ud130 \\ucef4\\ud4e8\\ud305 \\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.',\n",
       " u'Apache Spark Apache Spark Apache Spark Apache Spark',\n",
       " u'\\uc544\\ud30c\\uce58 \\uc2a4\\ud30c\\ud06c \\uc544\\ud30c\\uce58 \\uc2a4\\ud30c\\ud06c \\uc544\\ud30c\\uce58 \\uc2a4\\ud30c\\ud06c \\uc544\\ud30c\\uce58 \\uc2a4\\ud30c\\ud06c',\n",
       " u\"Originally developed at the University of California, Berkeley's AMPLab,\",\n",
       " u'the Spark codebase was later donated to the Apache Software Foundation,',\n",
       " u'which has maintained it since.',\n",
       " u'Spark provides an interface for programming entire clusters with',\n",
       " u'implicit data parallelism and fault-tolerance.']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아파 아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
      "아파 아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크\n",
      "-----\n",
      "im implicit data parallelism and fault-tolerance.\n",
      "-----\n",
      "th the Spark codebase was later donated to the Apache Software Foundation,\n",
      "-----\n",
      "Wi Wikipedia\n",
      "-----\n",
      "Ap Apache Spark is an open source cluster computing framework.\n",
      "Ap Apache Spark Apache Spark Apache Spark Apache Spark\n",
      "-----\n",
      "Sp Spark provides an interface for programming entire clusters with\n",
      "-----\n",
      "Or Originally developed at the University of California, Berkeley's AMPLab,\n",
      "-----\n",
      "wh which has maintained it since.\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "#myRdd_group=myRdd2.flatMap(lambda x:x.split()).groupBy(lambda x:w[0:2])\n",
    "#앞에 있는 두 글자를 key로 사용\n",
    "myRdd_group=myRdd2.groupBy(lambda x:x[0:2])\n",
    "\n",
    "for (k,v) in myRdd_group.collect():\n",
    "    for eachValue in v:\n",
    "        print k, eachValue\n",
    "    print \"-----\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Apache Spark is an open source cluster computing framework.\n",
      "A Apache Spark Apache Spark Apache Spark Apache Spark\n",
      "-----\n",
      "w which has maintained it since.\n",
      "-----\n",
      "i implicit data parallelism and fault-tolerance.\n",
      "-----\n",
      "O Originally developed at the University of California, Berkeley's AMPLab,\n",
      "-----\n",
      "S Spark provides an interface for programming entire clusters with\n",
      "-----\n",
      "W Wikipedia\n",
      "-----\n",
      "아 아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
      "아 아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크\n",
      "-----\n",
      "t the Spark codebase was later donated to the Apache Software Foundation,\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "#myRdd_group=myRdd2.flatMap(lambda x:x.split()).groupBy(lambda x:w[0:2])\n",
    "myRdd_group=myRdd2.groupBy(lambda x:x[0:1])\n",
    "\n",
    "for (k,v) in myRdd_group.collect():\n",
    "    for eachValue in v:\n",
    "        print k, eachValue\n",
    "    print \"-----\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "_testList=[(\"key1\",1),(\"key1\",1),(\"key1\",1),(\"key2\",1),(\"key2\",1),\n",
    "           (\"key1\",1),(\"key2\",1),\n",
    "           (\"key1\",1),(\"key1\",1),(\"key2\",1),(\"key2\",1)]\n",
    "_testRdd=spark.sparkContext.parallelize(_testList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key1', <pyspark.resultiterable.ResultIterable at 0x6268d68>),\n",
       " ('key2', <pyspark.resultiterable.ResultIterable at 0x6268048>)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.groupBy(lambda x:x[0]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pair RDD\n",
    "\n",
    "[] list\n",
    "() tuple\n",
    "{} dictimang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_testList=[(\"key1\",1),(\"key1\",1),(\"key1\",1),(\"key2\",1),(\"key2\",1),\n",
    "           (\"key1\",1),(\"key2\",1),\n",
    "           (\"key1\",1),(\"key1\",1),(\"key2\",1),(\"key2\",1)]\n",
    "_testRdd=spark.sparkContext.parallelize(_testList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['key1',\n",
       " 'key1',\n",
       " 'key1',\n",
       " 'key2',\n",
       " 'key2',\n",
       " 'key1',\n",
       " 'key2',\n",
       " 'key1',\n",
       " 'key1',\n",
       " 'key2',\n",
       " 'key2']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.keys().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key1', 6), ('key2', 5)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.reduceByKey(lambda x,y:x+y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key1', <pyspark.resultiterable.ResultIterable at 0x72cb860>),\n",
       " ('key2', <pyspark.resultiterable.ResultIterable at 0x72cb4a8>)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key1', [1, 1, 1, 1, 1, 1]), ('key2', [1, 1, 1, 1, 1])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.groupByKey().mapValues(list).collect()\n",
    "# list is a function, that is, list() key에 적용되는 것이아니라 value에 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key1', 2),\n",
       " ('key1', 2),\n",
       " ('key1', 2),\n",
       " ('key2', 2),\n",
       " ('key2', 2),\n",
       " ('key1', 2),\n",
       " ('key2', 2),\n",
       " ('key1', 2),\n",
       " ('key1', 2),\n",
       " ('key2', 2),\n",
       " ('key2', 2)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.mapValues(lambda x:x+1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key1', 2),\n",
       " ('key1', 2),\n",
       " ('key1', 2),\n",
       " ('key2', 2),\n",
       " ('key2', 2),\n",
       " ('key1', 2),\n",
       " ('key2', 2),\n",
       " ('key1', 2),\n",
       " ('key1', 2),\n",
       " ('key2', 2),\n",
       " ('key2', 2)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.mapValues(lambda x:x+1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에서 한 번 사용했던\n",
    "myRdd2=spark.sparkContext.textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'and', <pyspark.resultiterable.ResultIterable at 0x72a2390>),\n",
       " (u'\\uc18c\\uc2a4', <pyspark.resultiterable.ResultIterable at 0x72a2668>),\n",
       " (u'is', <pyspark.resultiterable.ResultIterable at 0x72a2518>)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#파이플라잉 형식\n",
    "myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\ # 2차원 이상을 1차원으로 만들어 주는 것이므로\n",
    "    .map(lambda x:(x,1))\\ # map 함수는 for문이 포함 돼, 1개씩 가져와서 라는 의미\n",
    "    .groupByKey()\\\n",
    "    .take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'and', 1),\n",
       " (u'\\uc18c\\uc2a4', 1),\n",
       " (u'is', 1),\n",
       " (u'Wikipedia', 1),\n",
       " (u'AMPLab,', 1),\n",
       " (u'maintained', 1),\n",
       " (u'donated', 1),\n",
       " (u'\\ucef4\\ud4e8\\ud305', 1),\n",
       " (u'open', 1),\n",
       " (u'since.', 1),\n",
       " (u'for', 1),\n",
       " (u'\\ud074\\ub7ec\\uc2a4\\ud130', 1),\n",
       " (u'with', 1),\n",
       " (u'framework.', 1),\n",
       " (u'provides', 1),\n",
       " (u'Apache', 6),\n",
       " (u'Spark', 7),\n",
       " (u'was', 1),\n",
       " (u'Originally', 1),\n",
       " (u'which', 1)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .groupByKey()\\  # key는 숫자로 설정할 수 없다.\n",
    "    .mapValues(sum)\\\n",
    "    .take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'AMPLab,', 1),\n",
       " (u'Apache', 6),\n",
       " (u\"Berkeley's\", 1),\n",
       " (u'California,', 1),\n",
       " (u'Foundation,', 1),\n",
       " (u'Originally', 1),\n",
       " (u'Software', 1),\n",
       " (u'Spark', 7),\n",
       " (u'University', 1),\n",
       " (u'Wikipedia', 1)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x): return len(x)\n",
    "myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(f)\\\n",
    "    .sortByKey(True)\\\n",
    "    .take(10)\n",
    "# 단어 빈도 word freq. NLP = word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc=myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(sum)\\\n",
    "    .sortByKey(True)\\\n",
    "    .take(10)\n",
    "# RDD가 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'AMPLab,', 1)\n",
      "(u'Apache', 6)\n",
      "(u\"Berkeley's\", 1)\n",
      "(u'California,', 1)\n",
      "(u'Foundation,', 1)\n",
      "(u'Originally', 1)\n",
      "(u'Software', 1)\n",
      "(u'Spark', 7)\n",
      "(u'University', 1)\n",
      "(u'Wikipedia', 1)\n"
     ]
    }
   ],
   "source": [
    "for e in wc:\n",
    "    print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'and', 1),\n",
       " (u'\\uc18c\\uc2a4', 1),\n",
       " (u'is', 1),\n",
       " (u'Wikipedia', 1),\n",
       " (u'AMPLab,', 1),\n",
       " (u'maintained', 1),\n",
       " (u'donated', 1),\n",
       " (u'\\ucef4\\ud4e8\\ud305', 1),\n",
       " (u'open', 1),\n",
       " (u'since.', 1)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .reduceByKey(lambda x,y:x+y)\\ #mapValues가 사용되지 않아도 된다.\n",
    "    .take(10)\n",
    "# List가 반환."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {u'AMPLab,': 1,\n",
       "             u'Apache': 6,\n",
       "             u\"Berkeley's\": 1,\n",
       "             u'California,': 1,\n",
       "             u'Foundation,': 1,\n",
       "             u'Originally': 1,\n",
       "             u'Software': 1,\n",
       "             u'Spark': 7,\n",
       "             u'University': 1,\n",
       "             u'Wikipedia': 1,\n",
       "             u'an': 2,\n",
       "             u'and': 1,\n",
       "             u'at': 1,\n",
       "             u'cluster': 1,\n",
       "             u'clusters': 1,\n",
       "             u'codebase': 1,\n",
       "             u'computing': 1,\n",
       "             u'data': 1,\n",
       "             u'developed': 1,\n",
       "             u'donated': 1,\n",
       "             u'entire': 1,\n",
       "             u'fault-tolerance.': 1,\n",
       "             u'for': 1,\n",
       "             u'framework.': 1,\n",
       "             u'has': 1,\n",
       "             u'implicit': 1,\n",
       "             u'interface': 1,\n",
       "             u'is': 1,\n",
       "             u'it': 1,\n",
       "             u'later': 1,\n",
       "             u'maintained': 1,\n",
       "             u'of': 1,\n",
       "             u'open': 1,\n",
       "             u'parallelism': 1,\n",
       "             u'programming': 1,\n",
       "             u'provides': 1,\n",
       "             u'since.': 1,\n",
       "             u'source': 1,\n",
       "             u'the': 3,\n",
       "             u'to': 1,\n",
       "             u'was': 1,\n",
       "             u'which': 1,\n",
       "             u'with': 1,\n",
       "             u'\\uc18c\\uc2a4': 1,\n",
       "             u'\\uc2a4\\ud30c\\ud06c': 4,\n",
       "             u'\\uc2a4\\ud30c\\ud06c\\ub294': 1,\n",
       "             u'\\uc544\\ud30c\\uce58': 5,\n",
       "             u'\\uc624\\ud508': 1,\n",
       "             u'\\ucef4\\ud4e8\\ud305': 1,\n",
       "             u'\\ud074\\ub7ec\\uc2a4\\ud130': 1,\n",
       "             u'\\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.': 1})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .countByKey() # .items() to be added to get a list\n",
    "# reduseByKey와 반환값이 다르다. dictionary가 반환\n",
    "# defaultdict { key : valus;단어의 빈도 수가 들어간다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/ds_bigdata_wiki.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_bigdata_wiki.txt\n",
    "Big data\n",
    "활용사례 및 의의[편집]\n",
    "정치 및 사회[편집]\n",
    "2008년 미국 대통령 선거[편집]\n",
    "2008년 미국 대통령 선거에서 버락 오바마 미국 대통령 후보는 다양한 형태의 유권자 데이터베이스를 확보하여 이를 분석, 활용한 '유권자 맞춤형 선거 전략'을 전개했다. 당시 오바마 캠프는 인종, 종교, 나이, 가구형태, 소비수준과 같은 기본 인적 사항으로 유권자를 분류하는 것을 넘어서서 과거 투표 여부, 구독하는 잡지, 마시는 음료 등 유권자 성향까지 전화나 개별 방문을 또는 소셜 미디어를 통해 유권자 정보를 수집하였다. 수집된 데이터는 오바마 캠프 본부로 전송되어 유권자 데이터베이스를 온라인으로 통합관리하는 ‘보트빌더(VoteBuilder.com)’시스템의 도움으로 유권자 성향 분석, 미결정 유권자 선별 , 유권자에 대한 예측을 해나갔다. 이를 바탕으로‘유권자 지도’를 작성한 뒤 ‘유권자 맞춤형 선거 전략’을 전개하는 등 오바마 캠프는 비용 대비 효과적인 선거를 치를 수 있었다.\n",
    "\n",
    "대한민국 제19대 총선[편집]\n",
    "중앙선거관리위원회는 대한민국 제19대 총선부터 소셜 네트워크 등 인터넷 상의 선거 운동을 상시 허용하였다.[15] 이에 소셜 미디어 상에서 선거 관련 데이터는 증폭되었으며, 2010년 대한민국 제5회 지방 선거 및 2011년 대한민국 재보궐선거에서 소셜 네트워크 서비스의 중요성을 확인한 정당들 또한 SNS 역량 지수를 공천 심사에 반영하는 등[16] 소셜 네트워크 활용에 주목했다. 이 가운데 여론 조사 기관들은 기존 여론조사 방식으로 예측한 2010년 제5회 지방 선거 및 2011년 재보궐선거의 여론조사 결과와 실제 투표 결과와의 큰 차이를 보완하고자 빅 데이터 기술을 활용한 SNS 여론 분석을 시행했다. 그러나 SNS 이용자의 대다수가 수도권 20~30대에 쏠려 있기에[17], 빅 데이터를 이용한 대한민국 제19대 총선에 대한 SNS 분석은 수도권으로 한정되어 일치하는 한계를 드러내기도 하였다.\n",
    "\n",
    "경제 및 경영[편집]\n",
    "아마존닷컴의 추천 상품 표시 / 구글 및 페이스북의 맞춤형 광고[편집]\n",
    "아마존닷컴은 모든 고객들의 구매 내역을 데이터베이스에 기록하고, 이 기록을 분석해 소비자의 소비 취향과 관심사를 파악한다.[18] 이런 빅 데이터의 활용을 통해 아마존은 고객별로 '추천 상품(레코멘데이션)'을 표시한다. 고객 한사람 한사람의 취미나 독서 경향을 찾아 그와 일치한다고 생각되는 상품을 메일, 홈 페이지상에서 중점적으로 고객 한사람 한사람에게 자동적으로 제시하는 것이다.[19] 아마존닷컴의 추천 상품 표시와 같은 방식으로 구글 및 페이스북도 이용자의 검색 조건, 나아가 사진과 동영상 같은 비정형 데이터 사용을 즉각 처리하여 이용자에게 맞춤형 광고를 제공하는 등 빅데이터의 활용을 증대시키고 있다.\n",
    "\n",
    "문화[편집]\n",
    "MLB (메이저 리그 베이스볼)의 머니볼 이론 및 데이터 야구[편집]\n",
    "머니볼 이론이란 경기 데이터를 철저하게 분석해 오직 데이터를 기반으로 적재적소에 선수들을 배치해 승률을 높인다는 게임 이론이다.[20] 이는 미국 메이저 리그 베이스볼 오클랜드 어슬레틱스의 구단장 빌리 빈이 리그 전체 25위에 해당하는 낮은 구단 지원금 속에서도 최소비용으로 최대효과를 거둔 상황에서 유래되었다. 빌리 빈은 하치해 최하위에 그치던 팀을 4년 연속 포스트시즌에 진출시키고 메이저 리그 최초로 20연승이라는 신기록을 세우도록 탈바꿈 시켰다. 미국 월스트리트 저널은 미국 경제에 큰 영향을 끼치는 파워 엘리트 30인에 워렌 버핏, 앨런 그린스펀과 함께 빌리 빈을 선정[21] 하는 등 머니볼 이론은 경영, 금융 분야에서도 주목받았다. 최근 들어서 과학기술 및 카메라 기술의 발달로 더욱 정교한 데이터의 수집이 가능해졌으며 투구의 궤적 및 투수의 그립, 타구 방향, 야수의 움직임까지 잡아낼 수 있게 되었다. 이처럼 기존의 정형 데이터뿐만 아닌 비정형 데이터의 수집과 분석, 활용을 통해 최근 야구경기에서 빅 데이터의 중요성은 더욱 커지고 있다.\n",
    "\n",
    "선수의 인기만을 쫓는 것이 아니라 팀별 승률이나 선수의 성적을 나타내는 수치와 야구를 관전한다면 그 재미는 배가된다. '출루율'은 타율로 인정되지 않는 볼넷을 포함하여 타자가 성공적으로 베이스를 밟은 횟수의 비율, '장타율'은 타수마다 밟은 총 베이스를 계산해서 타격력이 얼마나 강한지를 나타내는 비율이다.\n",
    "\n",
    "출루율과 장타율 못지 않게 '타수'는 한두 경기에서 낸 성적이 아닌, 수천 번의 타석에 들어 좋은 성적을 만들어낸 선수를 선별하기 위한 기초 통계자료이다. 이처럼 한 선수의 타율에서 팀의 역대 시리즈 전적까지 모든 것을 숫자로 표현할 수 있다고 해서 야구를 '통계의 스포츠'라고 부르기도 한다. 야구뿐만 아니라 생활 곳곳에서 활용되는 통계는 복잡한 상황과 설명을 간단한 숫자로 바꿔주는 매우 강력한 도구이다.[22]\n",
    "\n",
    "'프로파일링'과 '빅데이터' 기법을 활용한 프로그램 MBC <프로파일링>[편집]\n",
    "방송에는 19세 소년의 살인 심리를 파헤친 '용인살인사건의 재구성', 강남 3구 초등학교 85곳의 학업성취도평가 성적과 주변 아파트 매매가의 상관관계를 빅데이터(디지털 환경에서 발생한 방대한 규모의 데이터)를 통해 분석한 '강남, 부자일수록 공부를 잘할까'[23]\n",
    "\n",
    "2014년 FIFA 월드컵 독일 우승과 '빅데이터'[편집]\n",
    "브라질에서 개최된 2014년 FIFA 월드컵에서 독일은 준결승에서 개최국인 브라질을 7:1로 꺾고, 결승에서 아르헨티나와 연장전까지 가는 접전 끝에 1:0으로 승리를 거두었다. 무패행진으로 우승을 차지한 독일 국가대표팀의 우승의 배경에는 '빅데이터'가 있었다.\n",
    "\n",
    "독일 국가대표팀은 SAP와 협업하여 훈련과 실전 경기에 'SAP 매치 인사이트'를 도입했다. SAP 매치 인사이트란 선수들에게 부착된 센서를 통해 운동량, 순간속도, 심박수, 슈팅동작 등 방대한 비정형 데이터를 수집, 분석한 결과를 감독과 코치의 태블릿PC로 전송하여 그들이 데이터를 기반으로 전술을 짜도록 도와주는 솔루션이다. 기존에 감독의 경험이나 주관적 판단으로 결정되는 전략과는 달리, SAP 매치 인사이트를 통해 이루어지는 분석은 선수들에 대한 분석 뿐만 아니라 상대팀 전력, 강점, 약점 등 종합적인 분석을 통해 좀 더 과학적인 전략을 수립할 수 있다. 정보 수집에 쓰이는 센서 1개가 1분에 만들어내는 데이터는 총 12000여개로 독일 국가대표팀은 선수당 4개(골키퍼는 양 손목을 포함해 6개)의 센서를 부착했고, 90분 경기동안 한 선수당 약 432만개, 팀 전체로 약 4968만개의 데이터를 수집했다고 한다.월드컵8강 獨 전차군단 비밀병기는 '빅데이터'\n",
    "\n",
    "과학기술 및 활용[편집]\n",
    "통계학[편집]\n",
    "데이터 마이닝이란 기존 데이터베이스 관리도구의 데이터 수집, 저장, 관리, 분석의 역량을 넘어서는 대량의 정형 또는 비정형 데이터 집합 및 이러한 데이터로부터 가치를 추출하고 결과를 분석하는 기술로, 수집되는 ‘빅 데이터’를 보완하고 마케팅, 시청률조사, 경영 등으로부터 체계화돼 분류, 예측, 연관분석 등의 데이터 마이닝을 거쳐 통계학적으로 결과를 도출해 내고 있다.[24][25]\n",
    "\n",
    "대한민국에서는 2000년부터 정보통신부의 산하단체로 사단법인 한국BI데이터마이닝학회가 설립되어 데이터 마이닝에 관한 학술과 기술을 발전, 보급, 응용하고 있다. ‎또한 국내ㆍ외 통계분야에서 서서히 빅 데이터 활용에 대한 관심과 필요성이 커지고 있는 가운데 국가통계 업무를 계획하고 방대한 통계자료를 처리하는 국가기관인 통계청이 빅 데이터를 연구하고 활용방안을 모색하기 위한 '빅 데이터 연구회'를 발족하였다.[26] 하지만 업계에 따르면, 미국과 영국, 일본 등 선진국들은 이미 빅 데이터를 다각적으로 분석해 조직의 전략방향을 제시하는 데이터과학자 양성에 사활을 걸고 있다. 그러나 한국은 정부와 일부 기업이 데이터과학자 양성을 위한 프로그램을 진행 중에 있어 아직 걸음마 단계인 것으로 알려져 있다.[27]\n",
    "\n",
    "생물정보학[편집]\n",
    "최근 생물학에서 DNA, RNA, 단백질 서열 및 유전자들의 발현과 조절에 대한 데이터의 양이 급격히 증가했고 이에 따라 이 빅 데이터를 활용한 생명의 이해에 관한 논의가 진행되고 있다.\n",
    "\n",
    "보건의료[편집]\n",
    "국민건강보험공단은 가입자의 자격·보험료, 진료·투약내용, 건강검진 결과 및 생활습관 정보 등 2조1천억건, 92테라바이트의 빅데이터를 보유하고 있고, 한편, 건강보험심사평가원은 진료내역, 투약내용(의약품 안심서비스), 의약품 유통 등의 2조2천억건, 89테라바이트의 빅데이터를 보유하고 있으며, 경제협력개발기구(OECD)는 한국의 건강보험 빅데이터 순위가 2위라고 발표했었다. 건보공단과 심평원은 빅데이터를 민간에 널리 알리고 더 많이 개방하고 있다. (연합뉴스 2016.6.14 인터넷뉴스 참조)\n",
    "\n",
    "빅 데이터를 활용하면 미국 의료부문은 연간 3,300 억 달러(미 정부 의료 예산의 약 8%에 해당하는 규모)의 직간접적인 비용 절감 효과를 보일 것으로 전망된다.[28] 특히 임상분야에서는 의료기관 별 진료방법, 효능, 비용 데이터를 분석하여 보다 효과적인 진료방법을 파악하고 환자 데이터를 온라인 플랫폼화하여 의료협회 간 데이터 공유로 치료 효과를 제고하며 공중보건 영역에선 전국의 의료 데이터를 연계하여 전염병 발생과 같은 긴박한 순간에 빠른 의사결정을 가능케 할 전망이다.[29]\n",
    "\n",
    "한편, 의료 분야에서 빅 데이터가 효과를 발휘하기 위해서는 대량의 의료정보 수집이 필수적이기 때문에, 개인정보의 보호와 빅 데이터 활용이라는 두 가지 가치가 상충하게 되된다. 따라서, 의료 분야에서 빅 데이터의 활용과 보급을 위해서는 이러한 문제에 대한 가이드라인 마련이 필요한 상태이다.[30]\n",
    "\n",
    "기업 경영[편집]\n",
    "대규모의 다양한 데이터를 활용한 '빅데이터 경영'이 주목받으면서 데이터 품질을 높이고 방대한 데이터의 처리를 돕는 데이터 통합(Data Integration)의 중요성이 부각되고 있다.\n",
    "\n",
    "데이터 통합(DI)은 데이터의 추출, 변환, 적재를 위한 ETL 솔루션이 핵심인데 ETL 솔루션을 활용하면 일일이 수많은 데이터를 기업 데이터 포맷으로 코딩하지 않아도 되고 데이터 품질을 제고할 수 있기 때문에 DI는 빅데이터 환경에 꼭 필요한 데이터 솔루션으로 평가받고 있는 단계까지 진입되었다.\n",
    "\n",
    "한편 비즈니스 인텔리전스(Business Intelligence, BI)보다 진일보한 빅데이터 분석 방법이 비즈니스 애널리틱스(Business analytics, BA)인데 고급분석 범주에 있는 BA는 기본적으로 BI를 포함하면서도 미래 예측 기능과 통계분석, 확률 분석 등을 포함해 최적의 데이터 기반 의사결정을 가능케 하는 것으로 평가받고 있기도 하다.[31]\n",
    "\n",
    "마케팅[편집]\n",
    "인터넷으로 시작해서 인터넷으로 마감하는 생활, 스마트폰을 이용해 정보를 검색하고 쇼핑도하고 SNS를 이용해서 실시간으로 글을 남기는 등의 다양하게 인터넷을 이용하는 동안 남는 흔적같은 모인 데이터들을 분석하면 개인의 생활 패턴, 소비성향 등을 예측할 수 있고 기업들은 이런 데이터를 통해서 소비자가 원하는 것들을 미리 예측할 수 있다. 빅 데이터가 마케팅 자료로 활용되는 사례이다.[31]\n",
    "\n",
    "기상정보[편집]\n",
    "한반도 전역의 기상관측정보를 활용해 일기예보와 각종 기상특보 등 국가 기상서비스를 제공하고 있는 기상청은 정밀한 기상예측을 위한 분석 과정에서 발생하는 데이터 폭증에 대응하고자 빅데이터 저장시스템의 도입을 추진하였다.\n",
    "\n",
    "대다수 스토리지 기업들의 솔루션을 검토한 끝에 한국 IBM의 고성능 대용량 파일공유시스템(General Parallel File System, 이하 GPFS)을 적용한 스토리지 시스템을 선택하였다고 밝혔다.\n",
    "\n",
    "한국IBM이 기상청에 제공한 GPFS 기반의 빅데이터 저장시스템은 IBM 시스템 스토리지 제품군, 시스템 x서버 제품군과 고속 네트워킹 랙스위치(RackSwitch) 등이 통합돼 있는 시스템이다.[31]\n",
    "\n",
    "보안관리[편집]\n",
    "보안관리는 빅데이터 환경을 이용해 성장과 기술 발전을 동시에 이루는 분야로 분리한다. 클라우드 및 모바일 환경으로 접어들면서 물리/가상화 IT 시스템의 복잡성이 더욱 높아지고 있어 유무선 네트워크, 프라이빗/퍼블릭 클라우드, 모바일 애플리케이션과 기기관리 등 IT 시스템 전반에서 대대적인 변화가 예상되고 있어 막대한 양의 보안관리가 중요한 요소로 현실화되고 있다.[32]\n",
    "\n",
    "구글 번역[편집]\n",
    "구글에서 제공하는 자동 번역 서비스인 구글 번역은 빅 데이터를 활용한다. 지난 40년 간 컴퓨터 회사 IBM의 자동 번역 프로그램 개발은 컴퓨터가 명사, 형용사, 동사 등 단어와 어문의 문법적 구조를 인식하여 번역하는 방식으로 이뤄졌다. 이와 달리 2006년 구글은 수억 건의 문장과 번역문을 데이터베이스화하여 번역시 유사한 문장과 어구를 기존에 축적된 데이터를 바탕으로 추론해 나가는 통계적 기법을 개발하였다. 캐나다 의회의 수백만 건의 문서를 활용하여 영어-불어 자동번역 시스템개발을 시도한 IBM의 자동 번역 프로그램은 실패한 반면 구글은 수억 건의 자료를 활용하여 전 세계 58개 언어 간의 자동번역 프로그램 개발에 성공하였다. 이러한 사례로 미루어 볼 때, 데이터 양의 측면에서의 엄청난 차이가 두 기업의 자동 번역 프로그램의 번역의 질과 정확도를 결정했으며, 나아가 프로젝트의 성패를 좌우했다고 볼 수 있다.[31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/ds_wikipedia_wiki.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_wikipedia_wiki.txt\n",
    "배경\n",
    "위키백과 이전에도 온라인 백과사전을 운영하려는 여러 시도들이 있었으나 성공하지 못했다.[6] 위키백과 영어판은 전문가들이 작성했던 백과사전인 누피디아(지금은 없어짐)에서 비롯하였다.[2] 누피디아는 웹 포털 회사인 보미스가 2000년 3월 9일 개시하였다. 보미스의 CEO였던 지미 웨일스와 편집장 래리 생어는 누피디아의 글들을 오픈 콘텐츠로 제시하기로 하였고 리처드 스톨먼이 주도한 GNU 자유 문서 라이선스로 제공하였다.[7] 누피디아아는 그리 성공적이지 않았고, 지미 웨일스와 래리 생어는 누구나 참여할 수 있는 백과사전으로[8] 위키백과를 개설하였다.[9] 생어는 모두의 백과사전이라는 목표를 분명히 하기 위해 이름에 위키를 넣었다.[10] 2001년 1월 10일, 생어는 누피디어 메일링 리스트를 통해 누피디어 프로젝트를 보완하기 위해 위키를 도입한다고 밝혔다.[11]\n",
    "\n",
    "출범과 성장\n",
    "위키백과는 2001년 1월 15일 서비스를 개시하였다. 도메인은 www.wikipedia.com을 사용하였고 사용 언어는 영어 하나뿐이었다.[12] 래리 생어는 위키백과의 출범 소식 역시 누피디어 메일링 리스트를 통하여 알렸다.[8] 위키백과가 시작된 지 한 달 안에 중립성 정책이 수립되었다.[13] 이후 몇 가지 정책이 수립되면서 위키백과는 누피디아와는 별개인 서비스가 되었다.[8] 애초에 보미스는 위키백과를 영리 목적으로 운영하려고 하였다.[14]\n",
    "\n",
    "위키백과의 초기 편집자들은 누피디아, 슬래시닷, 그리고 웹 검색 엔진을 통해 유입되었다. 2001년 8월 8일 위키백과의 문서수는 약 8,000 개가 되었다.[15] 2001년 말이 되자 위키백과는 18개 언어판으로 늘었고, 문서 수는 2만여 개까지 늘어났다. 위키백과를 서비스 하는 언어는 2002년 말에는 26개, 2003년 말에는 46개, 2004년 말에는 161개로 늘어났다.[16] 누피디아는 위키백과와 병립하여 운영되다가 컨텐츠를 위키백과로 넘기고 2003년 서버를 다운시켜 마감하였다.\n",
    "\n",
    "2002년 사용자 사이에서 위키백과의 광고 수주와 그에 따른 상업적 운용에 대한 우려가 커졌고, 이에 따라 스페인어 위키백과 사용자들은 위키백과 소스를 포크하여 별도의 위키백과인 엔시클로페디아 리브레(Enciclopedia Libre Universal en Español)를 개설하였다.[17] 이에 자극받은 지미 웨일스는 위키백과에 광고를 도입하지 않겠다고 선언하였고 도메인을 wikipedia.com에서 wikipedia.org로 변경하였다.[18]\n",
    "\n",
    "영어 위키백과의 증대 속도는 2007년 초 정점을 찍었고, 2009년 8월 3백만 문서를 넘겼다.[19] 위키백과 전체의 문서수는 2006년 가장 빠르게 늘어 매일 약 1,800 개의 문서가 새로 생겨났다. 그 뒤로 문서 증가 속도는 둔화되어 2013년의 경우 연평균으로 보았을 때 매일 약 800개의 문서가 새로 생겨났다.[20] 위키백과의 성장 둔화 원인에 대해 팰로앨토 연구소는 프로젝트의 품질이 고급화 되면서 변화에 대한 저항이 있다고 분석한 바 있다.[21] 성장 둔화에 대한 다른 분석으로는 \"낮은 가지에 달린 열매\"처럼 주제나 가치가 분명하여 쉽게 만들 수 있는 문서는 이미 다 만들어졌기 때문이라는 설명이 있다.[22][23][24]\n",
    "\n",
    "2009년 11월 스페인 마드리드의 후안 카를로스 국왕 대학교의 연구자는 2009년 1분기 동안 영어 위키백과가 49,000 여명의 기여자를 잃었다는 분석을 내놓았다. 2008년의 같은 기간에 줄어든 기여자 수가 4,900 여명이었던 것에 비해 열 배나 더 많은 수치였다.[25][26] 《월스트리트 저널》은 까다로워진 편집 지침의 증가가 이러한 경향을 이끌었다고 보도하였다.[27] 지미 웨일스는 이러한 연구가 잘못된 방법론에 의한 것이라며 분석 결과를 거부하였다.[28] 2년 뒤인 2011년 지미 웨일스는 한 인터뷰에서 기여자 감소를 인정하였지만, 2010년 6월의 \"최소 36,000 명의 편집자\"에서 2011년 6월 당시의 35,800 명의 편집자 사이의 격차는 그리 크지 않다고 주장하면서 위키백과 편집자의 수는 \"안정적이고 지속적\"이라고 말하였다.[29] 2013년 매사추세츠 공과대학교의 《테크놀로지 리뷰》에 실린 〈위키백과의 하락〉(The Decline of Wikipedia)은 지미 웨일스의 이러한 주장을 반박하고 있다. 이 글의 분석에 따르면 위키백과는 2007년 이후 위키백과 문서를 업데이트하고 교정하던 자원 편집자 가운데 3분의 1을 잃었으며, 편집자의 상당수는 사소한 편집만을 하는 것으로 나타났다.[30]《디 애틀랜틱》 2012년 7월호는 관리자의 수 역시 줄어들었다고 보도하였다.[31] 2013년 11월 25일 《뉴욕》의 캐서린 워드는 “여섯 번째로 많이 사용되는 웹싸이트인 위키백과가 내부 비판에 휩싸였다”는 기사를 내보냈다.[32]\n",
    "주요 이력\n",
    "\n",
    "위키백과 문서 수(모든 언어)[33]\n",
    "\n",
    "월 100회 이상 편집한 위키백과 사용자 수(모든 언어)[34]\n",
    "\n",
    "2007년 1월 위키백과는 처음으로 가장 인기있는 웹사이트 리스트 톱 10에 이름을 올렸다. 컴스코어는 위키백과의 연간 방문자를 4,290만 명으로 집계하며 9위로 올렸고, 《뉴욕타임즈》는 10로 올렸다. 애플은 11위로 집계하였다. 2006년도의 순위가 33위였던 것에 비하면 놀라운 부상이었다.[35] 2015년 3월 위키백과는 5위를 기록하였다.[36][37] 이 시기 순위는 알렉사 인터넷의 조사에 의한 것으로, 위키백과는 2014년 내내 매 월 8억 이상의 페이지 뷰를 기록하였다.[38]\n",
    "\n",
    "2012년 1월 영어 위키백과는 미국 의회의 온라인 저작권 침해 금지 법안(SOPA)와 지적 재산권 보호 법안(PIPA)의 제정 시도에 맞어 SOPA와 PIPA 반대 시위의 일환으로 24시간 블랙아웃 시위를 벌였다.[39]\n",
    "\n",
    "2014년 1월 20일 수보드 바르마(Subodh Varma)는 《이코노믹 타임즈》에 투고한 글을 통해 위키백과가 2012년 12월에서 2013년 12월 사이에 전체적으로 페이지 뷰가 10퍼센트에 달하는 2억 번 이상의 페이지뷰를 잃었다고 발표하였다. 주요 언어판에 따라 나누면 영어 위키백과의 페이지뷰 감소율은 12%, 독일어가 17%, 일본어는 9% 였다. 바르마는 \"만일 위키백과 운영자들이 통계 집계에 오류가 있다고 주장한다면 지난해 도입된 구글의 지식 그래프가 그 입을 다물게 할 것\"이라고 덧붙였다.[40] 뉴욕 대학교의 부교수 클레이 셔키는 지식 그래프가 다른 사이트들의 페이지뷰를 잠식하고 있는 것에 대해 \"검색 페이지에서 당신의 질문에 대한 답을 바로 볼 수 있는데 굳이 그 싸이트를 방문하겠는가?\"라고 반문하였다.[40]\n",
    "\n",
    "2016년 12월 위키백과는 가장 인기있는 웹사이트 리스트에 5위로 기록되었다.[41]\n",
    "\n",
    "위키백과의 문서 수는 2016년 12월 31일 기준으로 영어 위키백과 5백3십만 개 이상, 스웨덴어 위키백과와 세부아노어 위키백과가 3백7십만 개 이상, 독일어 위키백과 2백만 개 이상, 네덜란드어 위키백과와 프랑스어 위키백과가 1백8십만 개 이상 등의 순위를 보이고 있으며, 운영중인 295개의 모든 언어를 합하면 약 4천3백3십만 개 이상이 된다.[42][43]\n",
    "특징\n",
    "개방성\n",
    "\n",
    "폴란드어 위키백과의 문서를 담은 DVD, 2007년 7월 말에 나왔다.\n",
    "위키백과의 가장 큰 특징은 누구나 편집과 관리에 참여할 수 있다는 점이다.[주석 1] 인터넷을 통해 누구나 글을 고칠 수 있는 체계인 위키로 만들어져 있어 집단 지성적 특성을 가진다.[3] 개방성은 위키백과의 가장 큰 특징 가운데 하나로, 원칙적으로 사용자들은 누구든 거의 모든 문서를 새로 만들고 수정할 수 있다.\n",
    "\n",
    "그러나 이러한 강점은 동시에 악의적인 문서의 훼손이나 부정확한 내용의 수록에 취약하다는 약점이 되기도 한다. 위키백과 커뮤니티는 이러한 약점을 보완하기 위해 편집 규칙을 정하고 일부 문서에 대한 생성과 편집을 규제하고 있다.[주석 2] 2009년 이후 여러 언어 마다 위키백과 편집에 대한 커뮤니티의 규제가 강화되었다. 영어 위키백과는 대중적 관심이 높은 문서에 대한 편집을 위해서는 로그인이 필요하도록 하였고, 독일어 위키백과는 모든 문서에 대해 로그인 된 사용자만이 편집할 수 있도록 하였다.[44]\n",
    "\n",
    "과도한 편집 규제는 위키백과 성장의 걸림돌이라는 지적이 있고[27], 위키백과 커뮤니티 내에서도 과감한 편집은 위키백과의 기본 원칙 가운데 하나로 새로운 사용자를 포용하기 위해서라도 지켜져야 한다는 의견들이 있다.[45]\n",
    "\n",
    "한편, 위키백과 커뮤니티 내에 존재하는 편향으로 인해 여성을 비롯한 다양한 집단에 대한 개방이 부족하다는 지적도 있다. 2014년 8월 게이머게이트 논쟁에서 영어 위키백과의 중재위원회가 내린 5명의 여성주의 운동가 차단 결정은 위키백과의 개방성에 대한 많은 논란을 불러오기도 하였다.[46]\n",
    "\n",
    "수정과 검토\n",
    "위키백과의 문서들은 끊임없이 누군가에 의해 수정된다. 위키백과의 편집 시스템인 미디어위키는 다양한 방법으로 문서의 수정 사항을 사용자에게 알려주어 검토할 수 있도록 한다. 사용자는 문서의 역사를 확인하여 누가 언제 어떤 내용을 수정했는 지 확인할 수 있다. 만약 변경 내용이 악의적인 문서 훼손이라면 사용자는 이를 손쉽게 되돌릴 수 있다. 또한 사용자는 시스템이 제공하는 \"최근 바뀜\"과 \"주시문서 목록\" 등의 기능을 통해 문서의 변경 사항을 쉽게 파악할 수 있다. 이러한 기능들은 위키백과가 반달리즘으로부터 문서 훼손을 보호할 수 있도록 돕는다.[47]\n",
    "\n",
    "위키백과는 시작과 함께 문서의 신뢰성에 대한 의문이 따라다녔다. 누군가 보다 전문가적인 입장에서 사용자의 편집을 검토하고 제한할 수 있어야 한다는 주장이 늘 있다. 위키백과의 공동창립자인 래리 생어는 결국 이 문제로 인해 위키백과를 떠나 전문가의 검토를 거치는 시티즌디움을 창립하였다.[48] 위키백과 역시 몇 차례의 명백한 오류와 특정 집단의 의도적인 개입으로 완전한 개방 정책을 수정하지 않을 수 없었다. 오랫동안 《USA 투데이》의 편집장을 역임했던 존 시겐설러가 존 F. 케네디의 암살에 연루되었다는 거짓 정보가 위키백과에 올라온 사례는 오랫동안 위키백과 문서의 오류에 대한 사례로 거론되었고[48], 2016년 1월에는 스위스의 정보 기관 공무원이 수년에 걸쳐 약 5,500 건에 달하는 문서를 악의적으로 편집하였다가 아이피가 차단되는 일이 벌어지기도 하였다.[49]\n",
    "\n",
    "위키백과의 문서 품질은 사용자들의 지속적인 수정과 검토에 의해서 유지되고 향상된다. 2003년 안드레 시포릴리는 위키백과 컨텐츠의 유지는 파괴적 활동보다 창조적 활동량이 훨씬 많기 때문에 가능한 것이라는 분석을 내놓았다.[50] 그러나, 교묘한 거짓 정보는 매우 오랫동안 검토되지 못하고 남아있기도 한다. 영어 위키백과에서는 2005년 1월 31일 등재된 연쇄 강간범 잭 로비쇼라는 문서가 완전히 허구의 인물을 서술한 것이라는 것을 2015년이 되어서야 발견한 일도 있었다. 이 문서는 2015년 9월 3일 삭제되었다.[51]\n",
    "\n",
    "커뮤니티\n",
    "<nowiki />이 부분의 본문은 위키백과 공동체입니다.\n",
    "\n",
    "위키마니아 2016 행사에 참가한 위키백과 사용자들\n",
    "\n",
    "위키컨퍼런스 서울 2016에 참여한 위키백과 사용자들\n",
    "위키백과의 컨텐츠는 사용자들의 자발적인 참여로 이루어지기 때문에 사용자간의 소통이 매우 중요하다. 이를 위해 위키백과는 사랑방과 같은 커뮤니티 공간을 제공하고 있다. 또한 위키백과의 모든 문서에는 \"토론\" 탭이 있어서 사용자들 사이에 문서 개선을 위한 토론이 이루어지도록 하고 있다.[주석 3]\n",
    "\n",
    "위키백과 사용자들은 위키컨퍼런스와 같은 오프라인 모임을 통해 관심사를 공유하기도 한다.[52] 위키백과를 운영하고 있는 위키미디어 재단은 매년 세계적인 컨퍼런스인 위키마니아 행사를 갖고 있다. 2016년 위키마니아는 이탈리아의 에시노라리오에서 열렸다.[53] 위키마니아에서는 위키백과뿐만 아니라 위키미디어 재단이 운영하고 있는 위키미디어 공용, 위키데이터, 위키책, 위키문헌, 위키낱말사전과 같은 여러 자매 프로젝트의 주요 관심사도 함께 논의된다. 위키미디어 재단은 이들 여러 프로젝트의 활성화를 위해 세계 각지의 지부나 사용자 모임을 지원하는 사업도 하고 있다.[54][55] 대한민국에서는 한국위키미디어협회가 자발적 사용자 모임으로 활동중이다. 한국위키미디어협회는 2016년 1월 15일 위키백과 15주년 기념행사를 가졌다.[56]\n",
    "\n",
    "위키백과 커뮤니티는 종종 컬트 문화적인 것으로 묘사되지만[57], 그것이 늘 부정적인 면을 부각하는 것은 아니다.[58] 위키백과 사용자들은 훌륭한 활동에 대해 반스타를 부여하여 서로의 동기 유발을 하기도 한다.[59]\n",
    "\n",
    "위키백과는 사용자의 익명성을 보장한다.[60] 다중이 익명으로 참여한다고 하더라도 커뮤니티가 활력을 띄면 정보의 질은 꾸준히 향상된다.[61] 위키백과의 이러한 작업 방식은 크라우드 소싱이라는 이름으로 다른 분야에서도 시도되고 있다.[62] 그러나, 실제로 위키백과에 정보를 추가하는 사람들은 전체 사용자 가운데 극히 소수라는 연구가 있고[63], 로그인 하지 않은 사용자에 대해서는 위키백과 커뮤니티가 이등시민 취급을 한다는 비판도 있다.[64] 다트머스 대학교 연구진은 이를 검증하기 위한 2007년 연구에서 \"로그인 하지 않은 익명의 편집자나 기여 횟수가 적은 편집자의 활동 역시 로그인 사용자와 동등한 신뢰성을 보인다\"고 밝혔다.[65] 2009년 《비지니스 인사이더》의 편집인 헨리 블라젯은 위키백과 문서에 대한 표집 조사 결과 대다수의 문서가 \"아웃사이더\"에 의해 생성된 뒤 \"인사이더\"에 의해 완성된다고 분석하였다.[66]\n",
    "\n",
    "몇몇 언어의 위키백과 커뮤니티는 자체적으로 문서들을 엮어서 출판물을 제작하기도 하는데, 독일어 위키백과의 경우 독일어 위키백과의 문서를 모아 2004년에 CD로, 2005년, 2006년에는 DVD와 책으로 제작하였다.\n",
    "\n",
    "라이선스\n",
    "위키백과의 내용은 처음에는 GNU 자유 문서 사용 허가서 아래 배포되었으나 2009년 6월, 크리에이티브 커먼즈 저작자표시-동일조건변경허락 3.0 Unported 라이선스로 변경되었다.[67]\n",
    "\n",
    "관리\n",
    "위키백과는 특별한 위계가 없는 사용자들의 집단 활동이라는 점에서 아나키즘의 요소를 갖는 민주주의 체계로 평가되기도 한다.[68][69] 위키백과 내의 모든 문서는 직접 내용 편집에 참여한 사용자를 포함하여 어느 누구도 소유권을 주장할 수 없다.[70] 위키백과의 이러한 규칙은 커뮤니티가 공동으로 소유하는 가치에 대한 사적 이익 추구를 억제함으로써 공유지의 비극을 방지하고자 만들어졌다.[71]\n",
    "\n",
    "위키백과의 관리는 다섯 원칙의 정신과 이를 구현하기 위한 정책과 지침에 따라 이루어진다. 정책과 지침은 커뮤니티의 총의에 의해 수립되거나 수정된다. 총의의 개념은 2005년 찰스 메튜의 위키미디어 메일링 리스트[72]에서 설명된 바와 같이 단순한 만장일치가 아닌 현시점에서 커뮤니티가 내릴 수 있는 최선의 타협이다.[73] 위키백과의 커뮤니티는 각각의 언어마다 독립되어 있기 때문에, 언어판마다 총의는 다를 수 있다. 위키백과 초창기 가장 큰 논란은 문서의 중립성 확보였고, 이에 따라 위키백과가 시작된지 한 달 만에 중립적 시각이 정책으로 지정되었다.[13] 한국어 위키백과 역시 2004년 중립적 시각 정책을 도입하였다.[74] 문서와 커뮤니티의 성장에 따라 지침이 필요한 다양한 사안이 발생하였기 때문에 위키백과의 정책과 지침 역시 이에 대응할 수 있도록 다양하게 늘어났다.[주석 4]\n",
    "\n",
    "위키백과 사용자들 사이의 논쟁 또는 분쟁은 모두 위키백과 커뮤니티 안에서 해결된다. 사용자들 사이의 문제는 서로간의 토론을 통해 해결하는 것이 가장 바람직하지만, 문서의 훼손이나 악의적인 행위 등으로부터 선의의 편집 활동을 보호하기 위한 조치도 필요하다. 문서를 삭제하거나 악의적인 사용자를 차단하는 것과 같은 활동은 커뮤니티 안에서 충분히 신뢰할 수 있다고 평가받아 관리자로 선출 된 사용자가 실행한다.[75] 한편 사용자 사이의 논쟁은 중재위원회와 같은 기구를 통해 상호 조정을 이루기도 한다.[76]\n",
    "\n",
    "운영\n",
    "위키백과는 위키미디어 재단이 운영하는 위키미디어 프로젝트 가운데 하나이다. 위키미디어 프로젝트에는 위키백과 외에도 위키낱말사전, 위키책, 위키미디어 공용, 위키문헌, 위키인용집, 위키데이터 등이 있다.[77] 모든 위키미디어 프로젝트는 자발적으로 참여하는 사용자들의 커뮤니티에 의해 운영되며 위키미디어 재단은 이들 프로젝트의 유지, 소프트웨어와 하드웨어의 관리, 사용자 커뮤니티에 대한 지원과 같은 일들을 담당한다.\n",
    "\n",
    "위키미디어 재단\n",
    "<nowiki />이 부분의 본문은 위키미디어 재단입니다.\n",
    "\n",
    "위키미디어 재단 로고\n",
    "위키미디어 재단은 미국 캘리포니아주 샌프란시스코에 본부를 둔 비영리 기구로 위키백과를 비롯한 위키미디어 프로젝트의 유지를 위한 기금을 조성하고 호스팅하고 있다.[78] 위키백과가 시작된 지 2년 후인 2003년 6월 20일 플로리다 주 법인으로 설립되었으며 2007년 본부를 샌프란시코로 이전하였다. 2013년 귀속분 국세청 신고서에 따르면 재단의 기금 수익은 3천9백7십만 달러이고 지출된 경비는 2천9백만 달러이다. 또한 총 자산은 3천7백2십만 달러로 이 가운데 부채는 230만 달러이다.[79]\n",
    "\n",
    "2014년 5월 위키미디어 재단은 초대 사무국장 슈 가드너가 퇴임하고 2대 사무국장으로 라일라 트레티코프를 지명하였다.[80] 《월스트리트 저널》은 2014년 5월 1일자 보도를 통해 새로운 사무국장의 취임을 소개하면서 트레티코프의 “정보는 공기와 같이 자유를 좋아한다.”는 말을 인용하였다.[81] 2016년 6월 3대 사무국장으로 캐서린 마허가 취임하였다.[82] 마허는 위키미디어 프로젝트의 운영 방향에 대해 커뮤니티 내에서 상호 공감을 형성하는 것이 무엇보다 중요하다는 입장을 밝혔다.[83]\n",
    "\n",
    "소프트웨어\n",
    " 미디어위키 문서를 참고하십시오.\n",
    "위키백과의 운영 프로그램은 미디어위키이다. 오픈 소스로 배포되는 자유 소프트웨어인 미디어위키는 PHP 기반의 위키 소프트웨어로 MySQL 데이터베이스를 이용한다.[84] 위키백과 초기에는 펄로 작성된 유스모드위키를 사용하였으나 2002년 1월에 마그누스 만스커가 개발한 PHP와 MySQL 기반의 위키가 도입되었고[85], 다시 2002년 7월 리 다니엘 크로커가 개발한 미디어위키를 3세대 소프트웨어로 도입하였다. 미디어위키는 이후로도 여러차례 업데이트 되어 위키백과를 비롯한 여러 위키미디어 프로젝트를 운영하는 소프트웨어가 되었다.[86]\n",
    "\n",
    "하드웨어\n",
    "위키백과는 낮 시간을 기준으로 1초에 25,000~60,000페이지 요청을 수신한다.[87] 페이지 요청은 먼저 스퀴드 캐시 서버의 프론트엔드 계층으로 내보낸다.[88] 스퀴드 캐시가 처리할 수 없는 요청은 리눅스 가상 서버 소프트웨어를 실행하고 있는 부하 제어 서버로 내보낸다. 즉, 데이터베이스로부터 렌더링한 페이지를 보여 주기 위해 아파치 웹 서버들 가운데 하나로 요청을 내보낸다는 뜻이다. 웹 서버는 요청한 페이지를 전달하여 모든 언어판의 위키백과에 대한 페이지 렌더링을 수행한다. 속도를 더 빠르게 하기 위해 렌더링 된 페이지는 만료될 때까지 분산 메모리 캐시에 캐시 처리되며 이로써 대부분의 동일한 페이지 접근을 위해 페이지 렌더링을 완전히 생략할 수 있다.\n",
    "\n",
    "\n",
    "위키미디어 서버 시스템 구성도 (2010년 12월 28일 기준)\n",
    "현재 사용되고 있는 위키백과의 서버는 주로 우분투로 이루어진 리눅스 서버들의 컴퓨터 클러스터로 운영되고 있다.[89][90] 2009년을 기준으로 위키미디어 재단은 미국 플로리다주에 300대, 네덜란드 암스테르담에 44대의 서버를 운영하였다.[91] 2013년 1월 22일 위키백과는 중요 데이터를 미국의 데이터 센터 공기업인 에퀴닉스로 이전하였다.[92][93]\n",
    "\n",
    "자동 편집\n",
    "위키백과에서는 단순 반복적인 활동을 위해 봇으로 불리는 프로그램이 운영된다. 봇은 자주 혼동되는 오탈자를 바로잡거나 자동으로 생성될 수 있는 반복적인 문구의 삽입과 같은 일을 담당한다. 봇 역시 위키백과 커뮤니티의 사용자들이 작성하여 운영하며 잘못된 사용을 막기 위해 별도의 등록 절차를 거친다.[94]\n",
    "\n",
    "비판\n",
    " 위키백과에 대한 비판 문서를 참고하십시오.\n",
    "\n",
    "존 시겐설러는 위키백과를 \"허점 많고 무책임한 사이트\"라고 비판했다.[95]\n",
    "위키백과는 누구나 참여할 수 있기 때문에 편집자의 시각에 따라 누군가 악의적으로 잘못된 정보를 입력할 수 있고, 이에 따라 잘못된 정보가 퍼져나갈 수 있다는 점이 제기되어 왔다. 예를 들어 2005년 영어 위키백과에서는 존 시겐설러라는 미국의 전직 언론인이 존 F. 케네디 대통령의 암살에 관여했다는 잘못된 정보가 올려져 있었다는 점이 밝혀졌으며[95], 한 익명 사용자가 신바드라는 미국의 코미디언이 사망했다는 거짓 정보를 올려 인터넷 전반에 잘못된 소문이 퍼지기도 했다.[96]\n",
    "\n",
    "또한 위키백과는 미국 내의 보수주의자들로부터 자유주의적이라는 비판을 받아왔다. 이로 인해 컨서버피디아가 2006년에 개설되었다.[97]\n",
    "\n",
    "미 연구팀에 의하면, 영어판 위키백과에 등록된 문서 중 회사 관련 내용의 60% 정도가 잘못된 사실을 담고 있다는 연구결과가 나왔다. 연구팀의 교수는 이를 특정 회사들이 이미지를 긍정적으로 만들기 위해 위키백과에 회사에 유리한 내용을 삽입하기 때문인 것으로 추정하고 있다. [98]\n",
    "\n",
    "같이 보기\n",
    "한국어 위키백과\n",
    "언어별 위키백과 목록\n",
    "집단 지성\n",
    "274301 위키피디아"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(os.path.join(\"data\", \"ds_wikipedia_wiki.txt\"))\n",
    "d = dict()\n",
    "for sent in f.readlines():\n",
    "    for w in sent.split():\n",
    "        if w not in d:\n",
    "            d[w]=1\n",
    "        else:\n",
    "            d[w]=d[w]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "페이지 6\n",
      "이 5\n",
      "2009년 6\n",
      "가장 7\n",
      "또한 4\n",
      "잘못된 7\n",
      "위키백과에 7\n",
      "편집 6\n",
      "2014년 5\n",
      "커뮤니티는 4\n",
      "커뮤니티의 4\n",
      "여러 5\n",
      "영어 10\n",
      "때문에 4\n",
      "있는 10\n",
      "한 4\n",
      "사용자들 5\n",
      "2003년 4\n",
      "2013년 5\n",
      "생어는 6\n",
      "웹 4\n",
      "이러한 8\n",
      "2007년 6\n",
      "문서에 4\n",
      "누구나 4\n",
      "약 5\n",
      "위키백과 40\n",
      "편집자의 4\n",
      "통해 9\n",
      "수 26\n",
      "개 5\n",
      "있다. 12\n",
      "사이의 4\n",
      "커뮤니티 7\n",
      "재단은 6\n",
      "의해 5\n",
      "사용자들의 4\n",
      "사용자들은 4\n",
      "사용자 7\n",
      "문서가 4\n",
      "문서 11\n",
      "12월 4\n",
      "역시 7\n",
      "따라 7\n",
      "있도록 4\n",
      "프로젝트의 6\n",
      "위키백과의 31\n",
      "가운데 7\n",
      "커뮤니티가 4\n",
      "독일어 4\n",
      "위해 11\n",
      "위한 5\n",
      "1월 8\n",
      "이에 5\n",
      "존 5\n",
      "문서의 8\n",
      "2001년 4\n",
      "래리 4\n",
      "2016년 6\n",
      "같은 8\n",
      "위키백과는 20\n",
      "문서를 9\n",
      "대한 16\n",
      "대해 5\n",
      "위키미디어 21\n",
      "위키백과를 8\n",
      "2002년 4\n",
      "지미 6\n",
      "모든 7\n",
      "악의적인 4\n",
      "이를 5\n",
      "미국 4\n",
      "위키백과가 9\n"
     ]
    }
   ],
   "source": [
    "d1 = dict()\n",
    "for key, value in d.iteritems():\n",
    "    if value>3:\n",
    "        d1[key]=value\n",
    "        print key, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRdd3=spark.sparkContext.textFile(os.path.join(\"data\",\"ds_wikipedia_wiki.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "배경\n",
      "위키백과 이전에도 온라인 백과사전을 운영하려는 여러 시도들이 있었으나 성공하지 못했다.[6] 위키백과 영어판은 전문가들이 작성했던 백과사전인 누피디아(지금은 없어짐)에서 비롯하였다.[2] 누피디아는 웹 포털 회사인 보미스가 2000년 3월 9일 개시하였다. 보미스의 CEO였던 지미 웨일스와 편집장 래리 생어는 누피디아의 글들을 오픈 콘텐츠로 제시하기로 하였고 리처드 스톨먼이 주도한 GNU 자유 문서 라이선스로 제공하였다.[7] 누피디아아는 그리 성공적이지 않았고, 지미 웨일스와 래리 생어는 누구나 참여할 수 있는 백과사전으로[8] 위키백과를 개설하였다.[9] 생어는 모두의 백과사전이라는 목표를 분명히 하기 위해 이름에 위키를 넣었다.[10] 2001년 1월 10일, 생어는 누피디어 메일링 리스트를 통해 누피디어 프로젝트를 보완하기 위해 위키를 도입한다고 밝혔다.[11]\n"
     ]
    }
   ],
   "source": [
    "for i in myRdd3.take(2):\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc3=myRdd3.flatMap(lambda x:x.split(\" \")).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "배경 위키백과 이전에도 온라인 백과사전을 운영하려는 여러 시도들이 있었으나 성공하지\n"
     ]
    }
   ],
   "source": [
    "for i in wc3:\n",
    "    print i,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc3=myRdd3.map(lambda x:x.split(\" \")).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'\\ubc30\\uacbd'] [u'\\uc704\\ud0a4\\ubc31\\uacfc', u'\\uc774\\uc804\\uc5d0\\ub3c4', u'\\uc628\\ub77c\\uc778', u'\\ubc31\\uacfc\\uc0ac\\uc804\\uc744', u'\\uc6b4\\uc601\\ud558\\ub824\\ub294', u'\\uc5ec\\ub7ec', u'\\uc2dc\\ub3c4\\ub4e4\\uc774', u'\\uc788\\uc5c8\\uc73c\\ub098', u'\\uc131\\uacf5\\ud558\\uc9c0', u'\\ubabb\\ud588\\ub2e4.[6]', u'\\uc704\\ud0a4\\ubc31\\uacfc', u'\\uc601\\uc5b4\\ud310\\uc740', u'\\uc804\\ubb38\\uac00\\ub4e4\\uc774', u'\\uc791\\uc131\\ud588\\ub358', u'\\ubc31\\uacfc\\uc0ac\\uc804\\uc778', u'\\ub204\\ud53c\\ub514\\uc544(\\uc9c0\\uae08\\uc740', u'\\uc5c6\\uc5b4\\uc9d0)\\uc5d0\\uc11c', u'\\ube44\\ub86f\\ud558\\uc600\\ub2e4.[2]', u'\\ub204\\ud53c\\ub514\\uc544\\ub294', u'\\uc6f9', u'\\ud3ec\\ud138', u'\\ud68c\\uc0ac\\uc778', u'\\ubcf4\\ubbf8\\uc2a4\\uac00', u'2000\\ub144', u'3\\uc6d4', u'9\\uc77c', u'\\uac1c\\uc2dc\\ud558\\uc600\\ub2e4.', u'\\ubcf4\\ubbf8\\uc2a4\\uc758', u'CEO\\uc600\\ub358', u'\\uc9c0\\ubbf8', u'\\uc6e8\\uc77c\\uc2a4\\uc640', u'\\ud3b8\\uc9d1\\uc7a5', u'\\ub798\\ub9ac', u'\\uc0dd\\uc5b4\\ub294', u'\\ub204\\ud53c\\ub514\\uc544\\uc758', u'\\uae00\\ub4e4\\uc744', u'\\uc624\\ud508', u'\\ucf58\\ud150\\uce20\\ub85c', u'\\uc81c\\uc2dc\\ud558\\uae30\\ub85c', u'\\ud558\\uc600\\uace0', u'\\ub9ac\\ucc98\\ub4dc', u'\\uc2a4\\ud1a8\\uba3c\\uc774', u'\\uc8fc\\ub3c4\\ud55c', u'GNU', u'\\uc790\\uc720', u'\\ubb38\\uc11c', u'\\ub77c\\uc774\\uc120\\uc2a4\\ub85c', u'\\uc81c\\uacf5\\ud558\\uc600\\ub2e4.[7]', u'\\ub204\\ud53c\\ub514\\uc544\\uc544\\ub294', u'\\uadf8\\ub9ac', u'\\uc131\\uacf5\\uc801\\uc774\\uc9c0', u'\\uc54a\\uc558\\uace0,', u'\\uc9c0\\ubbf8', u'\\uc6e8\\uc77c\\uc2a4\\uc640', u'\\ub798\\ub9ac', u'\\uc0dd\\uc5b4\\ub294', u'\\ub204\\uad6c\\ub098', u'\\ucc38\\uc5ec\\ud560', u'\\uc218', u'\\uc788\\ub294', u'\\ubc31\\uacfc\\uc0ac\\uc804\\uc73c\\ub85c[8]', u'\\uc704\\ud0a4\\ubc31\\uacfc\\ub97c', u'\\uac1c\\uc124\\ud558\\uc600\\ub2e4.[9]', u'\\uc0dd\\uc5b4\\ub294', u'\\ubaa8\\ub450\\uc758', u'\\ubc31\\uacfc\\uc0ac\\uc804\\uc774\\ub77c\\ub294', u'\\ubaa9\\ud45c\\ub97c', u'\\ubd84\\uba85\\ud788', u'\\ud558\\uae30', u'\\uc704\\ud574', u'\\uc774\\ub984\\uc5d0', u'\\uc704\\ud0a4\\ub97c', u'\\ub123\\uc5c8\\ub2e4.[10]', u'2001\\ub144', u'1\\uc6d4', u'10\\uc77c,', u'\\uc0dd\\uc5b4\\ub294', u'\\ub204\\ud53c\\ub514\\uc5b4', u'\\uba54\\uc77c\\ub9c1', u'\\ub9ac\\uc2a4\\ud2b8\\ub97c', u'\\ud1b5\\ud574', u'\\ub204\\ud53c\\ub514\\uc5b4', u'\\ud504\\ub85c\\uc81d\\ud2b8\\ub97c', u'\\ubcf4\\uc644\\ud558\\uae30', u'\\uc704\\ud574', u'\\uc704\\ud0a4\\ub97c', u'\\ub3c4\\uc785\\ud55c\\ub2e4\\uace0', u'\\ubc1d\\ud614\\ub2e4.[11]']\n"
     ]
    }
   ],
   "source": [
    "for i in wc3:\n",
    "    print i,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "배경 위키백과 이전에도 온라인 백과사전을 운영하려는 여러 시도들이 있었으나 성공하지 못했다.[6] 위키백과 영어판은 전문가들이 작성했던 백과사전인 누피디아(지금은 없어짐)에서 비롯하였다.[2] 누피디아는 웹 포털 회사인 보미스가 2000년 3월 9일 개시하였다. 보미스의 CEO였던 지미 웨일스와 편집장 래리 생어는 누피디아의 글들을 오픈 콘텐츠로 제시하기로 하였고 리처드 스톨먼이 주도한 GNU 자유 문서 라이선스로 제공하였다.[7] 누피디아아는 그리 성공적이지 않았고, 지미 웨일스와 래리 생어는 누구나 참여할 수 있는 백과사전으로[8] 위키백과를 개설하였다.[9] 생어는 모두의 백과사전이라는 목표를 분명히 하기 위해 이름에 위키를 넣었다.[10] 2001년 1월 10일, 생어는 누피디어 메일링 리스트를 통해 누피디어 프로젝트를 보완하기 위해 위키를 도입한다고 밝혔다.[11]\n"
     ]
    }
   ],
   "source": [
    "for i in wc3:\n",
    "    for j in i:\n",
    "        print j,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [u'배경',u'및', u'등',u'그리',u'수',u'위해',u'통해','is','am','are','the','for','a']\n",
    "wc3_stop1 = myRdd3\\\n",
    "    .flatMap(lambda x: x.split(' '))\\\n",
    "    .filter(lambda x: x.lower() not in stopwords)\\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "위키백과\n",
      "이전에도\n",
      "온라인\n",
      "백과사전을\n",
      "운영하려는\n",
      "여러\n",
      "시도들이\n",
      "있었으나\n",
      "성공하지\n",
      "못했다.[6]\n"
     ]
    }
   ],
   "source": [
    "for i in wc3_stop1:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'\\ubc30\\uacbd', 1)\n",
      "(u'\\uc704\\ud0a4\\ubc31\\uacfc \\uc774\\uc804\\uc5d0\\ub3c4 \\uc628\\ub77c\\uc778 \\ubc31\\uacfc\\uc0ac\\uc804\\uc744 \\uc6b4\\uc601\\ud558\\ub824\\ub294 \\uc5ec\\ub7ec \\uc2dc\\ub3c4\\ub4e4\\uc774 \\uc788\\uc5c8\\uc73c\\ub098 \\uc131\\uacf5\\ud558\\uc9c0 \\ubabb\\ud588\\ub2e4.[6] \\uc704\\ud0a4\\ubc31\\uacfc \\uc601\\uc5b4\\ud310\\uc740 \\uc804\\ubb38\\uac00\\ub4e4\\uc774 \\uc791\\uc131\\ud588\\ub358 \\ubc31\\uacfc\\uc0ac\\uc804\\uc778 \\ub204\\ud53c\\ub514\\uc544(\\uc9c0\\uae08\\uc740 \\uc5c6\\uc5b4\\uc9d0)\\uc5d0\\uc11c \\ube44\\ub86f\\ud558\\uc600\\ub2e4.[2] \\ub204\\ud53c\\ub514\\uc544\\ub294 \\uc6f9 \\ud3ec\\ud138 \\ud68c\\uc0ac\\uc778 \\ubcf4\\ubbf8\\uc2a4\\uac00 2000\\ub144 3\\uc6d4 9\\uc77c \\uac1c\\uc2dc\\ud558\\uc600\\ub2e4. \\ubcf4\\ubbf8\\uc2a4\\uc758 CEO\\uc600\\ub358 \\uc9c0\\ubbf8 \\uc6e8\\uc77c\\uc2a4\\uc640 \\ud3b8\\uc9d1\\uc7a5 \\ub798\\ub9ac \\uc0dd\\uc5b4\\ub294 \\ub204\\ud53c\\ub514\\uc544\\uc758 \\uae00\\ub4e4\\uc744 \\uc624\\ud508 \\ucf58\\ud150\\uce20\\ub85c \\uc81c\\uc2dc\\ud558\\uae30\\ub85c \\ud558\\uc600\\uace0 \\ub9ac\\ucc98\\ub4dc \\uc2a4\\ud1a8\\uba3c\\uc774 \\uc8fc\\ub3c4\\ud55c GNU \\uc790\\uc720 \\ubb38\\uc11c \\ub77c\\uc774\\uc120\\uc2a4\\ub85c \\uc81c\\uacf5\\ud558\\uc600\\ub2e4.[7] \\ub204\\ud53c\\ub514\\uc544\\uc544\\ub294 \\uadf8\\ub9ac \\uc131\\uacf5\\uc801\\uc774\\uc9c0 \\uc54a\\uc558\\uace0, \\uc9c0\\ubbf8 \\uc6e8\\uc77c\\uc2a4\\uc640 \\ub798\\ub9ac \\uc0dd\\uc5b4\\ub294 \\ub204\\uad6c\\ub098 \\ucc38\\uc5ec\\ud560 \\uc218 \\uc788\\ub294 \\ubc31\\uacfc\\uc0ac\\uc804\\uc73c\\ub85c[8] \\uc704\\ud0a4\\ubc31\\uacfc\\ub97c \\uac1c\\uc124\\ud558\\uc600\\ub2e4.[9] \\uc0dd\\uc5b4\\ub294 \\ubaa8\\ub450\\uc758 \\ubc31\\uacfc\\uc0ac\\uc804\\uc774\\ub77c\\ub294 \\ubaa9\\ud45c\\ub97c \\ubd84\\uba85\\ud788 \\ud558\\uae30 \\uc704\\ud574 \\uc774\\ub984\\uc5d0 \\uc704\\ud0a4\\ub97c \\ub123\\uc5c8\\ub2e4.[10] 2001\\ub144 1\\uc6d4 10\\uc77c, \\uc0dd\\uc5b4\\ub294 \\ub204\\ud53c\\ub514\\uc5b4 \\uba54\\uc77c\\ub9c1 \\ub9ac\\uc2a4\\ud2b8\\ub97c \\ud1b5\\ud574 \\ub204\\ud53c\\ub514\\uc5b4 \\ud504\\ub85c\\uc81d\\ud2b8\\ub97c \\ubcf4\\uc644\\ud558\\uae30 \\uc704\\ud574 \\uc704\\ud0a4\\ub97c \\ub3c4\\uc785\\ud55c\\ub2e4\\uace0 \\ubc1d\\ud614\\ub2e4.[11]', 1)\n",
      "(u'', 1)\n"
     ]
    }
   ],
   "source": [
    "wc3=myRdd3\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .take(3)\n",
    "for i in wc3:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'\\ubc30\\uacbd', 1) (u'\\uc704\\ud0a4\\ubc31\\uacfc', 1) (u'\\uc774\\uc804\\uc5d0\\ub3c4', 1) (u'\\uc628\\ub77c\\uc778', 1) (u'\\ubc31\\uacfc\\uc0ac\\uc804\\uc744', 1) (u'\\uc6b4\\uc601\\ud558\\ub824\\ub294', 1) (u'\\uc5ec\\ub7ec', 1) (u'\\uc2dc\\ub3c4\\ub4e4\\uc774', 1) (u'\\uc788\\uc5c8\\uc73c\\ub098', 1) (u'\\uc131\\uacf5\\ud558\\uc9c0', 1)\n"
     ]
    }
   ],
   "source": [
    "wc3=myRdd3\\\n",
    "    .flatMap(lambda x:x.split(\" \"))\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .take(10)\n",
    "for i in wc3:\n",
    "    print i,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [u'배경',u'대한',u'및', u'등',u'그리',u'수',u'위해',u'통해',u'있는',u'이러한',u'같은']\n",
    "wc3=myRdd3\\\n",
    "    .flatMap(lambda x:x.split(\" \"))\\\n",
    "    .filter(lambda x: x.lower() not in stopwords)\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .reduceByKey(lambda x,y:x+y)\\\n",
    "    .map(lambda x:(x[1],x[0]))\\ #value를 sorting하기 위해서 하는 단어 빈도 순서대로 보기위함\n",
    "    .sortByKey(False)\\\n",
    "    .take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "46 \n",
      "40 위키백과\n",
      "31 위키백과의\n",
      "21 위키미디어\n",
      "20 위키백과는\n",
      "12 있다.\n",
      "11 문서\n",
      "10 영어\n",
      "9 위키백과가\n",
      "9 문서를\n",
      "8 위키백과를\n",
      "8 1월\n",
      "8 문서의\n",
      "7 모든\n",
      "7 사용자\n"
     ]
    }
   ],
   "source": [
    "print type(wc3)\n",
    "for i in wc3:\n",
    "    print i[0],i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 아웃풋으로 확인하려면 써야한다. 쓰지않으면 창이 뜬다.\n",
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "count = map(lambda x: x[0], wc3)\n",
    "word = map(lambda x: x[1], wc3)\n",
    "plt.barh(range(len(count)), count, color = 'grey')\n",
    "plt.yticks(range(len(count)), word)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combineByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "marks = spark.sparkContext.parallelize([('kim',86),('lim',87),('kim',75),\n",
    "                                      ('kim',91),('lim',78),('lim',92),\n",
    "                                      ('lim',79),('lee',99)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "marksByKey = marks.combineByKey(lambda value: (value,1),\n",
    "                             lambda x,value: (x[0]+value, x[1]+1),\n",
    "                             lambda x,y: (x[0]+y[0], x[1]+y[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kim', (252, 3)), ('lim', (336, 4)), ('lee', (99, 1))]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marksByKey.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "heights = spark.sparkContext.parallelize([\n",
    "        ('M',182.),('F',164.),('M',180.),('M',185.),('M',171.),('F',162.)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "heightsByKey = heights.combineByKey(lambda value: (value,1),\n",
    "                             lambda x,value: (x[0]+value, x[1]+1),\n",
    "                             lambda x,y: (x[0]+y[0], x[1]+y[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('M', (718.0, 4)), ('F', (326.0, 2))]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heightsByKey.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'M': 179.5, 'F': 163.0}\n"
     ]
    }
   ],
   "source": [
    "avgByKey = heightsByKey.map(lambda (label,(valSum,count)):\n",
    "                                (label,valSum/count))\n",
    "\n",
    "print avgByKey.collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s.7 spark-submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/ds_spark_rdd_hello.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_spark_rdd_hello.py\n",
    "# -*- coding: UTF-8 -*-\n",
    "import pyspark\n",
    "def doIt():\n",
    "    print \"---------RESULT-----------\"\n",
    "    print spark.version\n",
    "    spark.conf.set(\"spark.logConf\",\"false\")\n",
    "    rdd=spark.sparkContext.parallelize(range(1000), 10)\n",
    "    print \"mean=\",rdd.mean()\n",
    "    nums = spark.sparkContext.parallelize([1, 2, 3, 4])\n",
    "    squared = nums.map(lambda x: x * x).collect()\n",
    "    for num in squared:\n",
    "        print \"%i \" % (num)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    myConf=pyspark.SparkConf()\n",
    "    spark = pyspark.sql.SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"myApp\")\\\n",
    "        .config(conf=myConf)\\\n",
    "        .getOrCreate()\n",
    "    doIt()\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
