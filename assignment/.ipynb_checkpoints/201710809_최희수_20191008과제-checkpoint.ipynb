{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"]=os.path.join(os.path.expanduser(\"~\"),'spark-2.0.0-bin-hadoop2.7')\n",
    "os.environ[\"PYLIB\"]=os.path.join(os.environ[\"SPARK_HOME\"],'python','lib')\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.1-src.zip'))\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'pyspark.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config('spark.sql.warehouse.dir','C:\\Users\\qorgk\\code\\spark')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Df = spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('data/publicBike.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|              �뿩����|�뿩�Ǽ�|\n",
      "+--------------------+-----+\n",
      "|2018-01-01 00:00:...| 4950|\n",
      "|2018-01-02 00:00:...| 7136|\n",
      "|2018-01-03 00:00:...| 7156|\n",
      "|2018-01-04 00:00:...| 7102|\n",
      "|2018-01-05 00:00:...| 7705|\n",
      "|2018-01-06 00:00:...| 5681|\n",
      "|2018-01-07 00:00:...| 5220|\n",
      "|2018-01-08 00:00:...| 6309|\n",
      "|2018-01-09 00:00:...| 5988|\n",
      "|2018-01-10 00:00:...| 4476|\n",
      "|2018-01-11 00:00:...| 4337|\n",
      "|2018-01-12 00:00:...| 4401|\n",
      "|2018-01-13 00:00:...| 3756|\n",
      "|2018-01-14 00:00:...| 4675|\n",
      "|2018-01-15 00:00:...| 6993|\n",
      "|2018-01-16 00:00:...| 7421|\n",
      "|2018-01-17 00:00:...| 6990|\n",
      "|2018-01-18 00:00:...| 7054|\n",
      "|2018-01-19 00:00:...| 8329|\n",
      "|2018-01-20 00:00:...| 6148|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- �뿩����: timestamp (nullable = true)\n",
      " |-- �뿩�Ǽ�: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Df.show()\n",
    "Df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Df=Df.withColumnRenamed('�뿩����','date')\n",
    "Df=Df.withColumnRenamed('�뿩�Ǽ�','count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                date|count|\n",
      "+--------------------+-----+\n",
      "|2018-01-01 00:00:...| 4950|\n",
      "|2018-01-02 00:00:...| 7136|\n",
      "|2018-01-03 00:00:...| 7156|\n",
      "|2018-01-04 00:00:...| 7102|\n",
      "|2018-01-05 00:00:...| 7705|\n",
      "|2018-01-06 00:00:...| 5681|\n",
      "|2018-01-07 00:00:...| 5220|\n",
      "|2018-01-08 00:00:...| 6309|\n",
      "|2018-01-09 00:00:...| 5988|\n",
      "|2018-01-10 00:00:...| 4476|\n",
      "|2018-01-11 00:00:...| 4337|\n",
      "|2018-01-12 00:00:...| 4401|\n",
      "|2018-01-13 00:00:...| 3756|\n",
      "|2018-01-14 00:00:...| 4675|\n",
      "|2018-01-15 00:00:...| 6993|\n",
      "|2018-01-16 00:00:...| 7421|\n",
      "|2018-01-17 00:00:...| 6990|\n",
      "|2018-01-18 00:00:...| 7054|\n",
      "|2018-01-19 00:00:...| 8329|\n",
      "|2018-01-20 00:00:...| 6148|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Df.show()\n",
    "Df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_col = pyspark.sql.functions.split(Df['date'], '-')\n",
    "Df = Df.withColumn('year', split_col.getItem(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----+\n",
      "|                date|count|year|\n",
      "+--------------------+-----+----+\n",
      "|2018-01-01 00:00:...| 4950|2018|\n",
      "|2018-01-02 00:00:...| 7136|2018|\n",
      "|2018-01-03 00:00:...| 7156|2018|\n",
      "|2018-01-04 00:00:...| 7102|2018|\n",
      "|2018-01-05 00:00:...| 7705|2018|\n",
      "|2018-01-06 00:00:...| 5681|2018|\n",
      "|2018-01-07 00:00:...| 5220|2018|\n",
      "|2018-01-08 00:00:...| 6309|2018|\n",
      "|2018-01-09 00:00:...| 5988|2018|\n",
      "|2018-01-10 00:00:...| 4476|2018|\n",
      "|2018-01-11 00:00:...| 4337|2018|\n",
      "|2018-01-12 00:00:...| 4401|2018|\n",
      "|2018-01-13 00:00:...| 3756|2018|\n",
      "|2018-01-14 00:00:...| 4675|2018|\n",
      "|2018-01-15 00:00:...| 6993|2018|\n",
      "|2018-01-16 00:00:...| 7421|2018|\n",
      "|2018-01-17 00:00:...| 6990|2018|\n",
      "|2018-01-18 00:00:...| 7054|2018|\n",
      "|2018-01-19 00:00:...| 8329|2018|\n",
      "|2018-01-20 00:00:...| 6148|2018|\n",
      "+--------------------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Df.show()\n",
    "Df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|year|sum(count)|\n",
      "+----+----------+\n",
      "|2019|   1871935|\n",
      "|2018|  10124874|\n",
      "+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Df.groupBy('year').sum('count').show()            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 년도별 따릉이 대여건수 합계를 계산한 결과\n",
    "2018년은 10124874건, 2019년은 1871935건 대여됐다는 것을 알 수 있었다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
